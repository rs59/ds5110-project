{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File 04 - Month to Month SQL Comparison\n",
    "##### Group 12:\n",
    "\n",
    "##### Hannah Schmuckler, mmc4cv\n",
    "\n",
    "##### Rob Schwartz, res7cd\n",
    "\n",
    "In this file, we look into comparing users and their characteristics over two months. The months we chose are January and February 2020. For the purposes of our analysis, month 1 is January and Month 2 is February. We are predicting spend in month 2 based on characteristics from month 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Spark session and data schema\n",
    "\n",
    "We can specify more options in the SparkSession creator, but currently the options are at the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 168 ms, sys: 178 ms, total: 346 ms\n",
      "Wall time: 4.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "schema = \"`event_time` TIMESTAMP,`event_type` STRING,`product_id` INT,`category_id` BIGINT,`category_code` STRING,`brand` STRING,`price` FLOAT,`user_id` INT,`user_session` STRING\"\n",
    "ddl_schema = T._parse_datatype_string(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in dataframes for two months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.49 ms, sys: 99 µs, total: 3.59 ms\n",
      "Wall time: 1.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df1 = spark.read.schema(ddl_schema).csv(\"/project/ds5559/group12/raw_data/2020-01.csv\")\n",
    "df2 = spark.read.schema(ddl_schema).csv(\"/project/ds5559/group12/raw_data/2020-02.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit number of records in dataframes\n",
    "\n",
    "We can limit each dataframe to a smaller subset. Notably, the dataframe is arranged by time, so this is how the subset will be biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1=df1.limit(10000)\n",
    "df1.createOrReplaceTempView(\"r1\")\n",
    "\n",
    "# df2=df2.limit(10000)\n",
    "df2.createOrReplaceTempView(\"r2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how many users are the same\n",
    "\n",
    "##### Full dataset\n",
    "\n",
    "- Over all interactions from each month: 1,702,723 are the same users\n",
    "- This is out of 4,385,986 users in month 1 and 4,233,207 users in month 2\n",
    "- So about 38% to 40% of users are the same from month to month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.3 ms, sys: 15.6 ms, total: 28 ms\n",
      "Wall time: 2min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1702723"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"SELECT DISTINCT r1.user_id FROM r1 INNER JOIN r2 on r1.user_id=r2.user_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 27.8 ms, total: 27.8 ms\n",
      "Wall time: 15.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4385986"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"SELECT DISTINCT r1.user_id FROM r1\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 33.8 ms, total: 33.8 ms\n",
      "Wall time: 14.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4233207"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"SELECT DISTINCT r2.user_id FROM r2\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how many users made purchases in both months\n",
    "\n",
    "##### Full dataset\n",
    "\n",
    "- Over all interactions from each month: 93,209 are the same purchasers\n",
    "- This is out of 359,105 purchasers in month 1 and 392,356 purchasers in month 2\n",
    "- This is out of 4,385,986 users in month 1 and 4,233,207 users in month 2\n",
    "\n",
    "- So about 24%-26% of purchasing users are the same from month to month\n",
    "- And about 2% of any users purchase in both months\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.05 ms, sys: 33.4 ms, total: 34.5 ms\n",
      "Wall time: 24.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93209"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"\"\"SELECT DISTINCT r1.user_id FROM r1 INNER JOIN r2 on r1.user_id=r2.user_id WHERE r1.event_type=\"purchase\" and r2.event_type=\"purchase\" \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 537 µs, sys: 34.6 ms, total: 35.2 ms\n",
      "Wall time: 11.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "359105"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"\"\"SELECT DISTINCT r1.user_id FROM r1 WHERE r1.event_type=\"purchase\" \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 34.3 ms, total: 34.3 ms\n",
      "Wall time: 11.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "392356"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"\"\"SELECT DISTINCT r2.user_id FROM r2 WHERE r2.event_type=\"purchase\" \"\"\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See some similar user behavior\n",
    "\n",
    "Let's look at the similarity of products purchased between users in each month. Takes about 1m30s to run.\n",
    "\n",
    "We can see that many products purchased in Month 10 are in the same category as products purchased in Month 11. Lots of nulls tend to clog up the dataset, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----------------------------+----------+\n",
      "|uid      |month|category_code                |event_type|\n",
      "+---------+-----+-----------------------------+----------+\n",
      "|378879891|1    |construction.tools.light     |purchase  |\n",
      "|378879891|1    |sport.bicycle                |purchase  |\n",
      "|378879891|2    |furniture.kitchen.table      |purchase  |\n",
      "|378879891|2    |apparel.scarf                |purchase  |\n",
      "|378879891|2    |furniture.living_room.cabinet|purchase  |\n",
      "|383787337|1    |construction.tools.light     |purchase  |\n",
      "|383787337|2    |construction.tools.light     |purchase  |\n",
      "|393237889|1    |construction.tools.light     |purchase  |\n",
      "|393237889|2    |construction.tools.light     |purchase  |\n",
      "|404851685|1    |electronics.video.tv         |purchase  |\n",
      "+---------+-----+-----------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 3.09 ms, sys: 30.1 ms, total: 33.2 ms\n",
      "Wall time: 47.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"\"\"SELECT uid, \"1\" AS month, category_code, event_type FROM (\n",
    "             SELECT DISTINCT r1.user_id AS uid FROM r1 INNER JOIN r2 ON r1.user_id=r2.user_id WHERE r1.event_type=\"purchase\" and r2.event_type=\"purchase\"\n",
    "              ) LEFT JOIN r1 ON uid=r1.user_id WHERE r1.event_type=\"purchase\"\n",
    "              \n",
    "              UNION ALL\n",
    "              \n",
    "              SELECT uid, \"2\" AS month, category_code, event_type FROM (\n",
    "              SELECT DISTINCT r1.user_id AS uid FROM r1 INNER JOIN r2 ON r1.user_id=r2.user_id WHERE r1.event_type=\"purchase\" and r2.event_type=\"purchase\"\n",
    "              ) LEFT JOIN r2 ON uid=r2.user_id WHERE r2.event_type=\"purchase\"\n",
    "              \n",
    "              ORDER BY uid, month ASC\n",
    "              \n",
    "              \"\"\").show(10,False)\n",
    "# spark.sql(\"SELECT DISTINCT r1.user_id FROM r1 INNER JOIN r2 on r1.user_id=r2.user_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# spark.sql(\"DROP TABLE IF EXISTS r_all\")\n",
    "# spark.sql(\"CREATE TABLE r_all LIKE r1\").count()\n",
    "# spark.sql(\"INSERT INTO r_all TABLE r1\")\n",
    "# spark.sql(\"INSERT INTO r_all TABLE r2\")\n",
    "# spark.sql(\"SELECT * FROM r_all\").count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
