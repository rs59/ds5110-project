{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File 02 - Feature Creation, test/train split, PCA\n",
    "##### Group 12:\n",
    "\n",
    "##### Hannah Schmuckler, mmc4cv\n",
    "\n",
    "##### Rob Schwartz, res7cd\n",
    "\n",
    "In this file, we create new features from our interaction-level dataset, handle obvious errors, and perform PCA. We will also create a downsampled dataset for comparison. We will output 4 files:\n",
    "\n",
    "- `processed_data/engineered_features.parquet` # This is just a raw file containing all engineered feature rows\n",
    "- `processed_data/train.parquet` #Our final training set\n",
    "- `processed_data/test.parquet` #Our final test set\n",
    "- `processed_data/ds_train.parquet` #Our downsampled training set - to see if downsampling might help performance. \n",
    "- `processed_data/ds_test.parquet` #Our test set - with PCA from the downsampled PCA. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Spark session and data schema\n",
    "\n",
    "We can specify more options in the SparkSession creator, but currently the options are at the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 632 ms, sys: 370 ms, total: 1 s\n",
      "Wall time: 6.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "import datetime as dt\n",
    "from pyspark.sql.functions import translate\n",
    "\n",
    "from pyspark.ml.feature import PCA as PCAml\n",
    "from pyspark.ml.linalg import Vectors \n",
    "              \n",
    "import copy\n",
    "    \n",
    "import sys\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#schema = \"`event_time` TIMESTAMP,`event_type` STRING,`product_id` INT,`category_id` BIGINT,`category_code` STRING,`brand` STRING,`price` FLOAT,`user_id` INT,`user_session` STRING\"\n",
    "#ddl_schema = T._parse_datatype_string(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://docs.google.com/document/d/1NG4KGticBXn0D3PL5_zMxLV2Pr7A8PQtLcasxCOd1nA/edit for table of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.98 ms, sys: 1.68 ms, total: 4.66 ms\n",
      "Wall time: 3.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full = spark.read.parquet(\"./processed_data/preprocessed_01.parquet\")\n",
    "m1 = spark.read.parquet(\"./processed_data/month_01_filtered.parquet\") # This brings in the data we can create additional features from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(full.count()) # 359105\n",
    "#full.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(m1.count()) #15923973\n",
    "#m1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Creating Features\n",
    "### Create each on an individual level, then join to full\n",
    "##### NOTE: Must rename all features so that they do not contain parenthesis - not compatible with saving to parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Session Duration (avg_session_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_ends = m1.groupBy('user_id', 'user_session').agg(max('event_time'), min('event_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session_ends.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_ends = session_ends.withColumn('session_length', (col(\"max(event_time)\").cast('long') - col(\"min(event_time)\").cast('long')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session_ends.orderBy(col(\"session_length\").desc()).show(5)\n",
    "# NOTE: Lots of these sessions are unreasonably long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sess = session_ends.groupBy('user_id').avg('session_length').withColumnRenamed('avg(session_length)', \"avg_session_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg_sess.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.join(avg_sess, full.user_id == avg_sess.user_id).drop(avg_sess.user_id)\n",
    "#print(full.count()) # 359105\n",
    "#full.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Std Deviation of session duration by person (sd_session_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session_ends.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_session_length = session_ends.groupBy('user_id') \\\n",
    "                                 .agg(stddev('session_length')) \\\n",
    "                                 .withColumnRenamed(\"stddev_samp(session_length)\", 'sd_session_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sd_session_length.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.join(sd_session_length, full.user_id == sd_session_length.user_id).drop(sd_session_length.user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average number of interactions per session (avg_interactions_per_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_per_session = m1.groupBy('user_id', 'user_session').agg(count('event_type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interactions_per_session.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_interactions_per_session = interactions_per_session.groupBy('user_id').avg('count(event_type)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_interactions_per_session = avg_interactions_per_session.withColumnRenamed('avg(count(event_type))', \"avg_interactions_per_session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.join(avg_interactions_per_session, full.user_id == avg_interactions_per_session.user_id).drop(avg_interactions_per_session.user_id)\n",
    "#full.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Std Deviation of number of interactions per session per person (sd_interactions_per_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_interactions_per_session = interactions_per_session.groupBy('user_id') \\\n",
    "                                                       .agg(stddev('count(event_type)')) \\\n",
    "                                                       .withColumnRenamed(\"stddev_samp(count(event_type))\", 'sd_interactions_per_session')\n",
    "#std_interactions_per_session.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.join(std_interactions_per_session, full.user_id == std_interactions_per_session.user_id).drop(std_interactions_per_session.user_id)\n",
    "#full.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max number of interactions within one session (max_interactions_per_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_interactions_per_session = interactions_per_session.groupBy('user_id').max('count(event_type)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_interactions_per_session = max_interactions_per_session.withColumnRenamed('max(count(event_type))', \"max_interactions_per_session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_interactions_per_session.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.join(max_interactions_per_session, full.user_id == max_interactions_per_session.user_id).drop(max_interactions_per_session.user_id)\n",
    "#full.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percent of total events that are x (Purchase, Cart, View) ('purchase_pct_of_total_events', 'cart_pct_of_total_events', 'view_pct_of_total_events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_counts = m1.groupBy('user_id', 'user_session').pivot('event_type').agg(count('event_type'))\n",
    "# Here the three types of event count are pivoted out for later tabulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_counts = event_counts.fillna(0) #replace nulls with 0 for math\n",
    "#event_counts.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_per_session = event_counts.withColumn('events_per_session_total', col('cart') + col('purchase') + col('view')) \n",
    "# Get total number of events per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#events_per_session.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_events = events_per_session.groupBy('user_id').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_totalevents = pct_events.withColumn('purchase_pct_of_total_events', col('sum(purchase)')/col('sum(events_per_session_total)')) \\\n",
    "                  .withColumn('view_pct_of_total_events', col('sum(view)')/col('sum(events_per_session_total)')) \\\n",
    "                  .withColumn('cart_pct_of_total_events', col('sum(cart)')/col('sum(events_per_session_total)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_me = pct_totalevents.select('user_id', 'purchase_pct_of_total_events', 'view_pct_of_total_events', 'cart_pct_of_total_events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.join(merge_me, full.user_id == merge_me.user_id).drop(merge_me.user_id)\n",
    "#full.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average number of purchases per session (avg_purchases_per_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_purchases_per_session = events_per_session.groupBy('user_id').avg('purchase').withColumnRenamed('avg(purchase)', \"avg_purchases_per_session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg_purchases_per_session.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.join(avg_purchases_per_session, full.user_id == avg_purchases_per_session.user_id).drop(avg_purchases_per_session.user_id)\n",
    "#full.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STD of number of purchases per session per person (sd_purchases_per_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_purchases_per_session = events_per_session.groupBy('user_id') \\\n",
    "                                              .agg(stddev('purchase')) \\\n",
    "                                              .withColumnRenamed('stddev_samp(purchase)', \"sd_purchases_per_session\")\n",
    "#std_purchases_per_session.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.join(std_purchases_per_session, full.user_id == std_purchases_per_session.user_id).drop(std_purchases_per_session.user_id)\n",
    "#full.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total number of each type of event over whole month (monthlyCartTotal, monthlyPurchaseTotal, monthlyViewTotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_counts_month = event_counts.groupBy('user_id').sum('cart', 'purchase', 'view')\\\n",
    "                     .withColumnRenamed('sum(cart)', 'cart_events') \\\n",
    "                     .withColumnRenamed('sum(purchase)', 'purchase_events') \\\n",
    "                     .withColumnRenamed('sum(view)', 'view_events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#event_counts_month.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.join(event_counts_month, full.user_id == event_counts_month.user_id).drop(event_counts_month.user_id)\n",
    "#full.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total number of sessions that contain event over whole month (sessions_with_purchase, sessions_with_cart, sessions_with_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_over_month = events_per_session.withColumn('purchase_events', when(col('purchase') == 0, 0).otherwise(1)) \\\n",
    "                                      .withColumn('cart_events', when(col('cart')==0, 0).otherwise(1)) \\\n",
    "                                      .withColumn('view_events', when(col('view')==0, 0).otherwise(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sesh_containing_event = events_over_month.groupBy('user_id').sum('purchase_events', \"cart_events\", \"view_events\") \\\n",
    "                            .withColumnRenamed(\"sum(purchase_events)\", \"sessions_with_purchase\") \\\n",
    "                            .withColumnRenamed(\"sum(cart_events)\", \"sessions_with_cart\") \\\n",
    "                            .withColumnRenamed(\"sum(view_events)\", \"sessions_with_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_sesh_containing_event.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.join(num_sesh_containing_event, full.user_id == num_sesh_containing_event.user_id).drop(num_sesh_containing_event.user_id)\n",
    "#full.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percent of individual's sessions that end in cart/purchase (pct_sessions_end_purchase, pct_sessions_end_cart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_ends2 = event_counts.withColumn('end_purchase', \\\n",
    "                                when(col('purchase') != 0, 1) \\\n",
    "                                .otherwise(0)) \\\n",
    "                            .withColumn('end_cart', \\\n",
    "                                when((col(\"purchase\") == 0) & (col(\"cart\") != 0), 1) \\\n",
    "                                .otherwise(0))\n",
    "#session_ends2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_sum = session_ends2.groupBy('user_id').agg(count('user_session'), sum('end_purchase'), sum('end_cart'))\n",
    "#session_sum.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_sum = session_sum.withColumn('pct_sessions_end_purchase', col('sum(end_purchase)')/col('count(user_session)')) \\\n",
    "                         .withColumn('pct_sessions_end_cart', col('sum(end_cart)')/col('count(user_session)'))\n",
    "#session_sum.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+---------------------+\n",
      "|  user_id|pct_sessions_end_purchase|pct_sessions_end_cart|\n",
      "+---------+-------------------------+---------------------+\n",
      "|519121308|      0.16666666666666666|  0.16666666666666666|\n",
      "|588954881|      0.09090909090909091|                  0.0|\n",
      "|568888698|       0.3076923076923077|  0.23076923076923078|\n",
      "|512700240|       0.5714285714285714|                  0.0|\n",
      "|512975406|       0.1346153846153846|  0.23076923076923078|\n",
      "+---------+-------------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = session_sum.select('user_id', \"pct_sessions_end_purchase\", \"pct_sessions_end_cart\")\n",
    "#temp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.join(temp, full.user_id == temp.user_id).drop(temp.user_id)\n",
    "#full.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add log of response column (T_total_spend_log) and total_spend_log, total_events_log, purchase_events_log, total_sessions_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full \\\n",
    "          .withColumn(\"total_spend_log\", log(col(\"total_spend\")+0.001)) \\\n",
    "          .withColumn(\"total_events_log\", log(col(\"total_events\")+0.001)) \\\n",
    "          .withColumn(\"purchase_events_log\", log(col(\"purchase_events\")+0.001)) \\\n",
    "          .withColumn(\"total_sessions_log\", log(col(\"total_sessions\")+0.001)) \\\n",
    "          .withColumn(\"T_total_spend_log\", log(col(\"T_total_spend\")+0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview full dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- T_total_spend: double (nullable = true)\n",
      " |-- total_spend: double (nullable = true)\n",
      " |-- total_events: long (nullable = true)\n",
      " |-- total_sessions: long (nullable = true)\n",
      " |-- avg_session_length: double (nullable = true)\n",
      " |-- sd_session_length: double (nullable = true)\n",
      " |-- avg_interactions_per_session: double (nullable = true)\n",
      " |-- sd_interactions_per_session: double (nullable = true)\n",
      " |-- max_interactions_per_session: long (nullable = true)\n",
      " |-- purchase_pct_of_total_events: double (nullable = true)\n",
      " |-- view_pct_of_total_events: double (nullable = true)\n",
      " |-- cart_pct_of_total_events: double (nullable = true)\n",
      " |-- avg_purchases_per_session: double (nullable = true)\n",
      " |-- sd_purchases_per_session: double (nullable = true)\n",
      " |-- cart_events: long (nullable = true)\n",
      " |-- purchase_events: long (nullable = true)\n",
      " |-- view_events: long (nullable = true)\n",
      " |-- sessions_with_purchase: long (nullable = true)\n",
      " |-- sessions_with_cart: long (nullable = true)\n",
      " |-- sessions_with_view: long (nullable = true)\n",
      " |-- pct_sessions_end_purchase: double (nullable = true)\n",
      " |-- pct_sessions_end_cart: double (nullable = true)\n",
      " |-- total_spend_log: double (nullable = true)\n",
      " |-- total_events_log: double (nullable = true)\n",
      " |-- purchase_events_log: double (nullable = true)\n",
      " |-- total_sessions_log: double (nullable = true)\n",
      " |-- T_total_spend_log: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+------------------+------------+--------------+------------------+------------------+----------------------------+---------------------------+----------------------------+----------------------------+------------------------+------------------------+-------------------------+------------------------+-----------+---------------+-----------+----------------------+------------------+------------------+-------------------------+---------------------+----------------+-----------------+-------------------+------------------+-----------------+\n",
      "|  user_id|     T_total_spend|       total_spend|total_events|total_sessions|avg_session_length| sd_session_length|avg_interactions_per_session|sd_interactions_per_session|max_interactions_per_session|purchase_pct_of_total_events|view_pct_of_total_events|cart_pct_of_total_events|avg_purchases_per_session|sd_purchases_per_session|cart_events|purchase_events|view_events|sessions_with_purchase|sessions_with_cart|sessions_with_view|pct_sessions_end_purchase|pct_sessions_end_cart| total_spend_log| total_events_log|purchase_events_log|total_sessions_log|T_total_spend_log|\n",
      "+---------+------------------+------------------+------------+--------------+------------------+------------------+----------------------------+---------------------------+----------------------------+----------------------------+------------------------+------------------------+-------------------------+------------------------+-----------+---------------+-----------+----------------------+------------------+------------------+-------------------------+---------------------+----------------+-----------------+-------------------+------------------+-----------------+\n",
      "|512372691|14188.679718017578|293.82000732421875|          66|             3| 539.3333333333334|144.67319493718708|                        11.0|          6.244997998398398|                          18|         0.09090909090909091|      0.8484848484848485|     0.06060606060606061|                      1.0|                     0.0|          2|              3|         28|                     3|                 2|                 3|                      1.0|                  0.0|5.68297076330826|4.189669893426794| 1.0989455664582302|1.0989455664582302|  9.5601997931763|\n",
      "+---------+------------------+------------------+------------+--------------+------------------+------------------+----------------------------+---------------------------+----------------------------+----------------------------+------------------------+------------------------+-------------------------+------------------------+-----------+---------------+-----------+----------------------+------------------+------------------+-------------------------+---------------------+----------------+-----------------+-------------------+------------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359105"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#full.count() 359105"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coerce NAs in standard deviation columnns to 0 (If there's only one, after all, the standard deviation IS 0!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.fillna(0, subset=['sd_session_length', 'sd_interactions_per_session', 'sd_purchases_per_session'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove errors/extreme outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at abnormally large number of sessions\n",
    "#full.select('total_sessions').sort(desc('total_sessions')).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359057"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More than 10 sessions a day for a month (300) seems likely to be an error. Removing those rows. \n",
    "full = full.filter(col('total_sessions') <= 300)\n",
    "#full.select('total_sessions').sort(desc('total_sessions')).show(5)\n",
    "#full.count() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at abnormally long sessions\n",
    "# Average session length greater than 24 hours,or 86400 seconds, is almost certainly an error.\n",
    "#full.select('avg_session_length').sort(desc('avg_session_length')).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351269"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove individuals with average session length greater than 24 hours (86400 seconds)\n",
    "full = full.filter(col('avg_session_length') <= 86400)\n",
    "#full.select('avg_session_length').sort(desc('avg_session_length')).show(5)\n",
    "#full.count() # 359057"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at outliers by total spend. \n",
    "#full.select('total_spend', 'purchase_events', 'T_total_spend').sort(desc('total_spend')).show(50)\n",
    "#full.select('total_spend').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "349358"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove those that spend > 100k a month... they probably aren't our 'usual' customer. \"Mega\" customers are a different analysis. \n",
    "full = full.filter(col('total_spend') <= 100000)\n",
    "#full.select('total_spend').sort(desc('total_spend')).show(5)\n",
    "#full.count() #349358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at abnormally large numbers of events\n",
    "# Greater than 3000 events is almost certainly a mistake. That's more than 100 events every day of the month\n",
    "#full.select('total_events').sort(desc('total_events')).show(50)\n",
    "#full.select('total_events').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "348356"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove extreme> 3000 events a month\n",
    "full = full.filter(col('total_events') <= 3000)\n",
    "#full.select('total_events').sort(desc('total_events')).show(5)\n",
    "#full.count() #348356"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save as parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.46 ms, sys: 4.49 ms, total: 13.9 ms\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full.write.mode(\"overwrite\").parquet(\"./processed_data/engineered_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.56 ms, sys: 1.1 ms, total: 2.65 ms\n",
      "Wall time: 28.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train, test = full.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After building our models using the original data, we became curious about how results would change if we downsampled the 0s in our response variable, since our data is zero-inflated. Below, we'll downsample the data. This new training set will be subbed in for the old, and original test set performance compared. (We have to do the downsampling here because otherwise we might wind up with overlap between it and the original test set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 361 µs, sys: 2.34 ms, total: 2.7 ms\n",
      "Wall time: 41.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "down_0 = train.filter(col('T_total_spend') == 0)\n",
    "down_1 = train.filter(col('T_total_spend') != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.74 ms, sys: 4.23 ms, total: 11 ms\n",
      "Wall time: 1min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "209502"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#down_0.count() #209502"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.07 ms, sys: 4.88 ms, total: 10.9 ms\n",
      "Wall time: 1min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69465"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#down_1.count() #69465"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks like there's about 3x as many 0s as there are non-0s. Downsample to even that out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 2.49 ms, total: 2.49 ms\n",
      "Wall time: 14.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "keep, _ = down_0.randomSplit([.33, .67], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.64 ms, sys: 4.57 ms, total: 10.2 ms\n",
      "Wall time: 1min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69343"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#keep.count() #69343"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join them back into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 547 µs, sys: 379 µs, total: 926 µs\n",
      "Wall time: 30.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_ds = down_1.union(keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.46 ms, sys: 4.67 ms, total: 14.1 ms\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_ds.write.mode(\"overwrite\").parquet(\"./processed_data/ds_engineered_features.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare PCA\n",
    "#### Purchased items in month 1, converted to PCA (pca_purchases)\n",
    "\n",
    "Note: Unlike all of the other preprocessing, we need to train the PCA model on the training set, then implement it on the test set. For this reason it comes after the train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 5 µs, total: 5 µs\n",
      "Wall time: 8.11 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create a function that prepares a dataset for PCA.\n",
    "\n",
    "def pca_prepare_on_subset(subset_df, limited_columns=[]):\n",
    "    # Only get this data from the training (or test) set\n",
    "    m1_subset = m1.join(subset_df,'user_id','leftsemi')\n",
    "\n",
    "    # Remove the periods from the dataframe category_code and replace with dashes. PySpark does not do well with periods in column\n",
    "    #  names, for some reason\n",
    "    m1_stripped = m1.withColumn('category_code_s', translate('category_code', '.', '-'))\n",
    "\n",
    "    # Pivot so that each category of purchase becomes a column. It will have the number of purchases a given person made in that category. \n",
    "    cats = m1_stripped.filter(m1.event_type == \"purchase\").groupBy('user_id').pivot('category_code_s').count().na.fill(0)\n",
    "\n",
    "    pca_input_cols = [cols for cols in cats.columns if cols!='user_id' and cols!='null']\n",
    "        \n",
    "    # Make a new copy of columns (this is from the training set to the test set, in order to filter out other columns)\n",
    "    if(limited_columns==[]):\n",
    "        limited_columns = copy.deepcopy(pca_input_cols)\n",
    "        limited_columns.append('user_id')\n",
    "    else:\n",
    "        cats = cats.select(*limited_columns) # This is for the test set \n",
    "        # print(cats.schema)\n",
    "\n",
    "    # Transform columns into a sparse vector (prepare for PCA)\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=pca_input_cols,\n",
    "        outputCol=\"to_pca_columns\")\n",
    "    \n",
    "    # Create sparse vector\n",
    "    pca_df = assembler.transform(cats)\n",
    "    return limited_columns, pca_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on Original Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------+\n",
      "|user_id  |to_pca_columns         |\n",
      "+---------+-----------------------+\n",
      "|512372691|(135,[0,112],[1.0,1.0])|\n",
      "|512517137|(135,[85],[1.0])       |\n",
      "+---------+-----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get columns from training set and get training df\n",
    "limited_columns, train_pre_pca = pca_prepare_on_subset(train)\n",
    "# Limit to these columns on the test set and get test df\n",
    "_, test_pre_pca = pca_prepare_on_subset(test, limited_columns=limited_columns)\n",
    "\n",
    "# Visualize what this looks like\n",
    "train_pre_pca.select([\"user_id\",\"to_pca_columns\"]).show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.9 ms, sys: 7.45 ms, total: 24.3 ms\n",
      "Wall time: 20 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create new PCA instance\n",
    "pca = PCAml(k=10, inputCol=\"to_pca_columns\", outputCol=\"pca_purchases10\")\n",
    "# Fit on training data\n",
    "model = pca.fit(train_pre_pca)\n",
    "\n",
    "# Transform training and test sets\n",
    "train_with_pca = model.transform(train_pre_pca)\n",
    "test_with_pca = model.transform(test_pre_pca)\n",
    "\n",
    "# Merge PCA df back into full training set\n",
    "join_train_df = train_with_pca.select([\"user_id\",\"pca_purchases10\"])\n",
    "train = train.join(join_train_df, train.user_id == join_train_df.user_id).drop(join_train_df.user_id)\n",
    "\n",
    "# Merge PCA df back into full test set\n",
    "join_test_df = test_with_pca.select([\"user_id\",\"pca_purchases10\"])\n",
    "test_o = test.join(join_test_df, test.user_id == join_test_df.user_id).drop(join_test_df.user_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.2 ms, sys: 6.05 ms, total: 22.3 ms\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create new PCA instance\n",
    "pca = PCAml(k=20, inputCol=\"to_pca_columns\", outputCol=\"pca_purchases20\")\n",
    "# Fit on training data\n",
    "model = pca.fit(train_pre_pca)\n",
    "\n",
    "# Transform training and test sets\n",
    "train_with_pca = model.transform(train_pre_pca)\n",
    "test_with_pca = model.transform(test_pre_pca)\n",
    "\n",
    "# Merge PCA df back into full training set\n",
    "join_train_df = train_with_pca.select([\"user_id\",\"pca_purchases20\"])\n",
    "train = train.join(join_train_df, train.user_id == join_train_df.user_id).drop(join_train_df.user_id)\n",
    "\n",
    "# Merge PCA df back into full test set\n",
    "join_test_df = test_with_pca.select([\"user_id\",\"pca_purchases20\"])\n",
    "test_o = test_o.join(join_test_df, test_o.user_id == join_test_df.user_id).drop(join_test_df.user_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.2 ms, sys: 6.13 ms, total: 23.3 ms\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create new PCA instance\n",
    "pca = PCAml(k=50, inputCol=\"to_pca_columns\", outputCol=\"pca_purchases50\")\n",
    "# Fit on training data\n",
    "model = pca.fit(train_pre_pca)\n",
    "\n",
    "# Transform training and test sets\n",
    "train_with_pca = model.transform(train_pre_pca)\n",
    "test_with_pca = model.transform(test_pre_pca)\n",
    "\n",
    "# Merge PCA df back into full training set\n",
    "join_train_df = train_with_pca.select([\"user_id\",\"pca_purchases50\"])\n",
    "train = train.join(join_train_df, train.user_id == join_train_df.user_id).drop(join_train_df.user_id)\n",
    "\n",
    "# Merge PCA df back into full test set\n",
    "join_test_df = test_with_pca.select([\"user_id\",\"pca_purchases50\"])\n",
    "test_o = test_o.join(join_test_df, test_o.user_id == join_test_df.user_id).drop(join_test_df.user_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.5 ms, sys: 2.18 ms, total: 22.7 ms\n",
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create new PCA instance\n",
    "pca = PCAml(k=100, inputCol=\"to_pca_columns\", outputCol=\"pca_purchases100\")\n",
    "# Fit on training data\n",
    "model = pca.fit(train_pre_pca)\n",
    "\n",
    "# Transform training and test sets\n",
    "train_with_pca = model.transform(train_pre_pca)\n",
    "test_with_pca = model.transform(test_pre_pca)\n",
    "\n",
    "# Merge PCA df back into full training set\n",
    "join_train_df = train_with_pca.select([\"user_id\",\"pca_purchases100\"])\n",
    "train = train.join(join_train_df, train.user_id == join_train_df.user_id).drop(join_train_df.user_id)\n",
    "\n",
    "# Merge PCA df back into full test set\n",
    "join_test_df = test_with_pca.select([\"user_id\",\"pca_purchases100\"])\n",
    "test_o = test_o.join(join_test_df, test_o.user_id == join_test_df.user_id).drop(join_test_df.user_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.show(5, truncate=False)\n",
    "#test_o.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.9 ms, sys: 10.2 ms, total: 23.1 ms\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train.write.mode(\"overwrite\").parquet(\"./processed_data/train.parquet\")\n",
    "test_o.write.mode(\"overwrite\").parquet(\"./processed_data/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on Downsampled Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------+\n",
      "|user_id  |to_pca_columns         |\n",
      "+---------+-----------------------+\n",
      "|512372691|(135,[0,112],[1.0,1.0])|\n",
      "|512517137|(135,[85],[1.0])       |\n",
      "+---------+-----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get columns from training set and get training df\n",
    "limited_columns, train_pre_pca = pca_prepare_on_subset(df_ds)\n",
    "# Limit to these columns on the test set and get test df\n",
    "_, test_pre_pca = pca_prepare_on_subset(test, limited_columns=limited_columns)\n",
    "\n",
    "# Visualize what this looks like\n",
    "train_pre_pca.select([\"user_id\",\"to_pca_columns\"]).show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.2 ms, sys: 7.47 ms, total: 19.6 ms\n",
      "Wall time: 9.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create new PCA instance\n",
    "pca = PCAml(k=10, inputCol=\"to_pca_columns\", outputCol=\"pca_purchases10\")\n",
    "# Fit on training data\n",
    "model = pca.fit(train_pre_pca)\n",
    "\n",
    "# Transform training and test sets\n",
    "train_with_pca = model.transform(train_pre_pca)\n",
    "test_with_pca = model.transform(test_pre_pca)\n",
    "\n",
    "# Merge PCA df back into full training set\n",
    "join_train_df = train_with_pca.select([\"user_id\",\"pca_purchases10\"])\n",
    "train_ds = df_ds.join(join_train_df, df_ds.user_id == join_train_df.user_id).drop(join_train_df.user_id)\n",
    "\n",
    "# Merge PCA df back into full test set\n",
    "join_test_df = test_with_pca.select([\"user_id\",\"pca_purchases10\"])\n",
    "test_ds = test.join(join_test_df, test.user_id == join_test_df.user_id).drop(join_test_df.user_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 ms, sys: 5.3 ms, total: 19.7 ms\n",
      "Wall time: 8.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create new PCA instance\n",
    "pca = PCAml(k=20, inputCol=\"to_pca_columns\", outputCol=\"pca_purchases20\")\n",
    "# Fit on training data\n",
    "model = pca.fit(train_pre_pca)\n",
    "\n",
    "# Transform training and test sets\n",
    "train_with_pca = model.transform(train_pre_pca)\n",
    "test_with_pca = model.transform(test_pre_pca)\n",
    "\n",
    "# Merge PCA df back into full training set\n",
    "join_train_df = train_with_pca.select([\"user_id\",\"pca_purchases20\"])\n",
    "train_ds = train_ds.join(join_train_df, train_ds.user_id == join_train_df.user_id).drop(join_train_df.user_id)\n",
    "\n",
    "# Merge PCA df back into full test set\n",
    "join_test_df = test_with_pca.select([\"user_id\",\"pca_purchases20\"])\n",
    "test_ds = test_ds.join(join_test_df, test_ds.user_id == join_test_df.user_id).drop(join_test_df.user_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.2 ms, sys: 4.5 ms, total: 18.7 ms\n",
      "Wall time: 9.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create new PCA instance\n",
    "pca = PCAml(k=50, inputCol=\"to_pca_columns\", outputCol=\"pca_purchases50\")\n",
    "# Fit on training data\n",
    "model = pca.fit(train_pre_pca)\n",
    "\n",
    "# Transform training and test sets\n",
    "train_with_pca = model.transform(train_pre_pca)\n",
    "test_with_pca = model.transform(test_pre_pca)\n",
    "\n",
    "# Merge PCA df back into full training set\n",
    "join_train_df = train_with_pca.select([\"user_id\",\"pca_purchases50\"])\n",
    "train_ds = train_ds.join(join_train_df, train_ds.user_id == join_train_df.user_id).drop(join_train_df.user_id)\n",
    "\n",
    "# Merge PCA df back into full test set\n",
    "join_test_df = test_with_pca.select([\"user_id\",\"pca_purchases50\"])\n",
    "test_ds = test_ds.join(join_test_df, test_ds.user_id == join_test_df.user_id).drop(join_test_df.user_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.6 ms, sys: 7.63 ms, total: 19.2 ms\n",
      "Wall time: 8.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create new PCA instance\n",
    "pca = PCAml(k=100, inputCol=\"to_pca_columns\", outputCol=\"pca_purchases100\")\n",
    "# Fit on training data\n",
    "model = pca.fit(train_pre_pca)\n",
    "\n",
    "# Transform training and test sets\n",
    "train_with_pca = model.transform(train_pre_pca)\n",
    "test_with_pca = model.transform(test_pre_pca)\n",
    "\n",
    "# Merge PCA df back into full training set\n",
    "join_train_df = train_with_pca.select([\"user_id\",\"pca_purchases100\"])\n",
    "train_ds = train_ds.join(join_train_df, train_ds.user_id == join_train_df.user_id).drop(join_train_df.user_id)\n",
    "\n",
    "# Merge PCA df back into full test set\n",
    "join_test_df = test_with_pca.select([\"user_id\",\"pca_purchases100\"])\n",
    "test_ds = test_ds.join(join_test_df, test_ds.user_id == join_test_df.user_id).drop(join_test_df.user_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ds.show(1, truncate=False)\n",
    "#test_ds.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write downsampled train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.8 ms, sys: 9.65 ms, total: 23.5 ms\n",
      "Wall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_ds.write.mode(\"overwrite\").parquet(\"./processed_data/ds_train.parquet\")\n",
    "test_ds.write.mode(\"overwrite\").parquet(\"./processed_data/ds_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
