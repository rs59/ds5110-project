{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: Should we depreciate? I don't think it's relevant anymore with the prefiltered data\n",
    "\n",
    "## File 02 - Month to Month SQL Comparison\n",
    "\n",
    "In this file, we look into comparing users and their characteristics over two months.\n",
    "NOTE: This is currently October 2019 and November 2019. In case we want to use this for anything in the future, we should change the months to unify with the months we are actually using. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Spark session and data schema\n",
    "\n",
    "We can specify more options in the SparkSession creator, but currently the options are at the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 216 ms, sys: 145 ms, total: 361 ms\n",
      "Wall time: 5.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "schema = \"`event_time` TIMESTAMP,`event_type` STRING,`product_id` INT,`category_id` BIGINT,`category_code` STRING,`brand` STRING,`price` FLOAT,`user_id` INT,`user_session` STRING\"\n",
    "ddl_schema = T._parse_datatype_string(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in dataframes for two months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.43 ms, sys: 1.56 ms, total: 5.99 ms\n",
      "Wall time: 1.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df10 = spark.read.option(\"header\",\"true\") \\\n",
    "        .schema(ddl_schema) \\\n",
    "        .csv(\"./processed_data/month_01_filtered.parquet\")\n",
    "\n",
    "df11 = spark.read.option(\"header\",\"true\") \\\n",
    "        .schema(ddl_schema) \\\n",
    "        .csv(\"./processed_data/month_02_filtered.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit number of records in dataframes\n",
    "\n",
    "We can limit each dataframe to a smaller subset. Notably, the dataframe is arranged by time, so this is how the subset will be biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df10=df10.limit(10000)\n",
    "df10.createOrReplaceTempView(\"r10\")\n",
    "\n",
    "# df11=df11.limit(10000)\n",
    "df11.createOrReplaceTempView(\"r11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how many users are the same\n",
    "\n",
    "##### Small sample\n",
    "\n",
    "- Over the first 100,000 records from each month: 340 are the same users\n",
    "- Over the first 1,000,000 records from each month: 11,891 are the same users\n",
    "\n",
    "##### Full dataset\n",
    "\n",
    "- Over all records from each month: 1,401,758 are the same users\n",
    "- This is out of 3,022,290 users in October and 3,696,117 users in November\n",
    "- So about 2/5 to 1/2 of users are the same from month to month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.06 ms, sys: 1.16 ms, total: 3.22 ms\n",
      "Wall time: 4.47 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"SELECT DISTINCT r10.user_id FROM r10 INNER JOIN r11 on r10.user_id=r11.user_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.21 ms, sys: 1.21 ms, total: 2.43 ms\n",
      "Wall time: 1.29 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"SELECT DISTINCT r10.user_id FROM r10\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.3 ms, sys: 1.3 ms, total: 2.59 ms\n",
      "Wall time: 1.13 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"SELECT DISTINCT r11.user_id FROM r11\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how many users made purchases in both months\n",
    "\n",
    "##### Full dataset\n",
    "\n",
    "- Over all records from each month: 91,286 are the same purchasers\n",
    "- This is out of 347,118 purchasers in October and 441,638 purchasers in November\n",
    "- This is out of 3,022,290 users in October and 3,696,117 users in November\n",
    "- So about 20%-24% of purchasers are the same from month to month\n",
    "- And about 2%-3% of purchasing users are the same from month to month\n",
    "\n",
    "This means that given a set of purchasing and non-purchasing users, we want to predict:\n",
    "- (a) which purchasers in October do and do not go on to purchase again and\n",
    "- (b) which non-purchasers in October do and do not go on to purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.04 ms, sys: 1.03 ms, total: 3.07 ms\n",
      "Wall time: 2.08 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"\"\"SELECT DISTINCT r10.user_id FROM r10 INNER JOIN r11 on r10.user_id=r11.user_id WHERE r10.event_type=\"purchase\" and r11.event_type=\"purchase\" \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.79 ms, sys: 0 ns, total: 2.79 ms\n",
      "Wall time: 958 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"\"\"SELECT DISTINCT r10.user_id FROM r10 WHERE r10.event_type=\"purchase\" \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.95 ms, sys: 381 µs, total: 2.33 ms\n",
      "Wall time: 683 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"\"\"SELECT DISTINCT r11.user_id FROM r11 WHERE r11.event_type=\"purchase\" \"\"\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See some similar user behavior\n",
    "\n",
    "Let's look at the similarity of products purchased between users in each month. Takes about 1m30s to run.\n",
    "\n",
    "We can see that many products purchased in Month 10 are in the same category as products purchased in Month 11. Lots of nulls tend to clog up the dataset, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------------+----------+\n",
      "|uid|month|category_code|event_type|\n",
      "+---+-----+-------------+----------+\n",
      "+---+-----+-------------+----------+\n",
      "\n",
      "CPU times: user 142 µs, sys: 3.2 ms, total: 3.35 ms\n",
      "Wall time: 3.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"\"\"SELECT uid, \"10\" AS month, category_code, event_type FROM (\n",
    "             SELECT DISTINCT r10.user_id AS uid FROM r10 INNER JOIN r11 ON r10.user_id=r11.user_id WHERE r10.event_type=\"purchase\" and r11.event_type=\"purchase\"\n",
    "              ) LEFT JOIN r10 ON uid=r10.user_id WHERE r10.event_type=\"purchase\"\n",
    "              \n",
    "              UNION ALL\n",
    "              \n",
    "              SELECT uid, \"11\" AS month, category_code, event_type FROM (\n",
    "              SELECT DISTINCT r10.user_id AS uid FROM r10 INNER JOIN r11 ON r10.user_id=r11.user_id WHERE r10.event_type=\"purchase\" and r11.event_type=\"purchase\"\n",
    "              ) LEFT JOIN r11 ON uid=r11.user_id WHERE r11.event_type=\"purchase\"\n",
    "              \n",
    "              ORDER BY uid, month ASC\n",
    "              \n",
    "              \"\"\").show(1000,False)\n",
    "# spark.sql(\"SELECT DISTINCT r10.user_id FROM r10 INNER JOIN r11 on r10.user_id=r11.user_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.51 ms, sys: 1.61 ms, total: 3.12 ms\n",
      "Wall time: 3.83 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# spark.sql(\"DROP TABLE IF EXISTS r_all\")\n",
    "# spark.sql(\"CREATE TABLE r_all LIKE r10\").count()\n",
    "# spark.sql(\"INSERT INTO r_all TABLE r10\")\n",
    "# spark.sql(\"INSERT INTO r_all TABLE r11\")\n",
    "# spark.sql(\"SELECT * FROM r_all\").count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
