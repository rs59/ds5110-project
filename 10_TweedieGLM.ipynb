{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File 09: GLM Regression with Tweedie\n",
    "\n",
    "Our response data is very 0-heavy. We createed a regression that accounts for this by using a Tweedie loss function, however, we were unable to get it to run with parameters that allowed it to create a mixture model. The error we got is 'sum of weights cannot be zero'. Our research suggests that this is an error in the pyspark implementation of the Tweedie/GLM model. It works for fake data, but the structure of ours causes it to break. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Spark session\n",
    "\n",
    "We can specify more options in the SparkSession creator, but currently the options are at the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 504 ms, sys: 425 ms, total: 930 ms\n",
      "Wall time: 5.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import log\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in dataframes for train and test sets\n",
    "\n",
    "This data should have been previously generated: we can find it in the `processed_data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-----------------+------------+--------------+------------------+------------------+----------------------------+---------------------------+----------------------------+----------------------------+------------------------+------------------------+-------------------------+------------------------+-----------+---------------+-----------+----------------------+------------------+------------------+-------------------------+---------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  user_id|     T_total_spend|      total_spend|total_events|total_sessions|avg_session_length| sd_session_length|avg_interactions_per_session|sd_interactions_per_session|max_interactions_per_session|purchase_pct_of_total_events|view_pct_of_total_events|cart_pct_of_total_events|avg_purchases_per_session|sd_purchases_per_session|cart_events|purchase_events|view_events|sessions_with_purchase|sessions_with_cart|sessions_with_view|pct_sessions_end_purchase|pct_sessions_end_cart|     pca_purchases10|     pca_purchases20|     pca_purchases50|    pca_purchases100|\n",
      "+---------+------------------+-----------------+------------+--------------+------------------+------------------+----------------------------+---------------------------+----------------------------+----------------------------+------------------------+------------------------+-------------------------+------------------------+-----------+---------------+-----------+----------------------+------------------+------------------+-------------------------+---------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|512424146|11270.879859924316|526.6399841308594|         288|             7| 260.7142857142857|262.34564622885983|           5.142857142857143|         2.4784787961282104|                           8|         0.08333333333333333|      0.7777777777777778|      0.1388888888888889|      0.42857142857142855|      0.5345224838248488|          5|              3|         28|                     3|                 4|                 7|      0.42857142857142855|  0.14285714285714285|[-0.0221225462344...|[-0.0221225462344...|[-0.0221225462344...|[-0.0221225462344...|\n",
      "|512440689|               0.0|640.9400024414062|         113|            16|           566.875| 711.0573230525557|                      7.0625|          6.060459278525569|                          17|        0.008849557522123894|      0.9734513274336283|    0.017699115044247787|                   0.0625|     0.24999999999999997|          2|              1|        110|                     1|                 1|                16|                   0.0625|                  0.0|[4.69075176148695...|[4.69075176148694...|[4.69075176148694...|[4.69075176148694...|\n",
      "|512467274| 1532.799949645996|39.90000057220459|          30|             6|39.166666666666664| 75.29785300170694|          1.6666666666666667|         1.2110601416389968|                           4|                         0.1|                     0.8|                     0.1|      0.16666666666666666|       0.408248290463863|          1|              1|          8|                     1|                 1|                 6|      0.16666666666666666|                  0.0|[-6.0647773396890...|[-6.0647773396890...|[-6.0647773396890...|[-6.0647773396890...|\n",
      "|512470083|               0.0|385.8500061035156|          16|             5|             101.0|  72.8800384193093|                         3.2|         1.3038404810405297|                           4|                      0.0625|                    0.75|                  0.1875|                      0.2|      0.4472135954999579|          3|              1|         12|                     1|                 3|                 5|                      0.2|                  0.4|[-0.0053959415183...|[-0.0053959415183...|[-0.0053959415183...|[-0.0053959415183...|\n",
      "|512474045|               0.0|370.6499938964844|          29|             6|237.16666666666666|229.47541625774787|           4.833333333333333|         2.7141603981096374|                           9|         0.06896551724137931|      0.8620689655172413|     0.06896551724137931|       0.3333333333333333|      0.5163977794943222|          2|              2|         25|                     2|                 2|                 6|       0.3333333333333333|                  0.0|[-1.9975955387030...|[-1.9975955387030...|[-1.9975955387030...|[-1.9975955387030...|\n",
      "+---------+------------------+-----------------+------------+--------------+------------------+------------------+----------------------------+---------------------------+----------------------------+----------------------------+------------------------+------------------------+-------------------------+------------------------+-----------+---------------+-----------+----------------------+------------------+------------------+-------------------------+---------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 3.82 ms, sys: 1.82 ms, total: 5.64 ms\n",
      "Wall time: 5.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainDF = spark.read.parquet(\"./processed_data/train.parquet\")\n",
    "testDF = spark.read.parquet(\"./processed_data/test.parquet\")\n",
    "trainDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF \\\n",
    "          .withColumn(\"total_spend_log\", log(col(\"total_spend\"))) \\\n",
    "          .withColumn(\"total_events_log\", log(col(\"total_events\"))) \\\n",
    "          .withColumn(\"purchase_events_log\", log(col(\"purchase_events\"))) \\\n",
    "          .withColumn(\"total_sessions_log\", log(col(\"total_sessions\"))) \\\n",
    "          .withColumn(\"T_total_spend_log\", log(col(\"T_total_spend\"))) \\\n",
    "          .withColumn(\"total_spend_pos\", (col(\"total_spend\"))) \\\n",
    "          .withColumn(\"total_events_pos\", (col(\"total_events\"))) \\\n",
    "          .withColumn(\"purchase_events_pos\", (col(\"purchase_events\"))) \\\n",
    "          .withColumn(\"total_sessions_pos\", (col(\"total_sessions\"))) \\\n",
    "          .withColumn(\"T_total_spend_pos\", (col(\"T_total_spend\")/100+1))\n",
    "\n",
    "testDF = testDF \\\n",
    "          .withColumn(\"total_spend_log\", log(col(\"total_spend\"))) \\\n",
    "          .withColumn(\"total_events_log\", log(col(\"total_events\"))) \\\n",
    "          .withColumn(\"purchase_events_log\", log(col(\"purchase_events\"))) \\\n",
    "          .withColumn(\"total_sessions_log\", log(col(\"total_sessions\"))) \\\n",
    "          .withColumn(\"T_total_spend_log\", log(col(\"T_total_spend\"))) \\\n",
    "          .withColumn(\"total_spend_pos\", (col(\"total_spend\"))) \\\n",
    "          .withColumn(\"total_events_pos\", (col(\"total_events\"))) \\\n",
    "          .withColumn(\"purchase_events_pos\", (col(\"purchase_events\"))) \\\n",
    "          .withColumn(\"total_sessions_pos\", (col(\"total_sessions\"))) \\\n",
    "          .withColumn(\"T_total_spend_pos\", (col(\"T_total_spend\")/100+1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Spark ML pipeline training for generalized linear regression\n",
    "\n",
    "Here we decide which input columns should be used in order to create our training pipeline. To implement this step, we create the function `generatePipeline(inputCols, outputCol`). Then, we train the pipeline using this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.9 ms, sys: 4.85 ms, total: 25.7 ms\n",
      "Wall time: 2.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inputCols = [\"total_spend\",\"total_events\", \"total_sessions\", \"avg_session_length\", \"avg_interactions_per_session\", \"max_interactions_per_session\",\n",
    "             \"purchase_pct_of_total_events\", \"view_pct_of_total_events\", \"cart_pct_of_total_events\",\"avg_purchases_per_session\", \"cart_events\", \"purchase_events\",\n",
    "             \"view_events\", \"sessions_with_purchase\", \"sessions_with_cart\",\"sessions_with_view\", \"pct_sessions_end_purchase\", \"pct_sessions_end_cart\", 'sd_session_length', \n",
    "             'sd_interactions_per_session', 'sd_purchases_per_session', \"total_spend_log\", \"total_events_log\", \"purchase_events_log\", \"total_sessions_log\"]\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.GeneralizedLinearRegression.html\n",
    "# on choosing variable values: https://www.rdocumentation.org/packages/statmod/versions/1.4.36/topics/tweedie\n",
    "# LinkPower: Supported variables:\n",
    "# variancePower: Supported values 0 and [1, inf)\n",
    "\n",
    "def generateGLRPipeline(inputCols, outputCol):\n",
    "    # Select input columns for generalized linear regression\n",
    "    vecAssembler = VectorAssembler(inputCols=inputCols, outputCol=\"features\")\n",
    "    \n",
    "    # create glr instance & select output col\n",
    "    glr = GeneralizedLinearRegression(featuresCol = \"features\", labelCol = outputCol, family = \"tweedie\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[vecAssembler, glr])\n",
    "    return pipeline\n",
    "    \n",
    "pipeline = generateGLRPipeline(inputCols, \"T_total_spend\")\n",
    "pipelineGLRModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients\n",
      "                     Column name   Coefficient\n",
      "0                      intercept  -5724.193740\n",
      "1                    total_spend      3.969475\n",
      "2                   total_events    221.150082\n",
      "3                 total_sessions   1479.286548\n",
      "4             avg_session_length     -0.100492\n",
      "5   avg_interactions_per_session   -243.084291\n",
      "6   max_interactions_per_session     88.166037\n",
      "7   purchase_pct_of_total_events   8699.050752\n",
      "8       view_pct_of_total_events  21138.891424\n",
      "9       cart_pct_of_total_events  13104.380189\n",
      "10     avg_purchases_per_session  -4571.903503\n",
      "11                   cart_events    -42.839087\n",
      "12               purchase_events  -2008.045178\n",
      "13                   view_events   -196.391913\n",
      "14        sessions_with_purchase  -2120.213019\n",
      "15            sessions_with_cart    777.519725\n",
      "16            sessions_with_view  -1151.966962\n",
      "17     pct_sessions_end_purchase -14646.054030\n",
      "18         pct_sessions_end_cart  -3652.196247\n",
      "19             sd_session_length      0.052860\n",
      "20   sd_interactions_per_session    -81.672612\n",
      "21      sd_purchases_per_session  -6260.200178\n",
      "22               total_spend_log   1456.848709\n",
      "23              total_events_log   1248.967883\n",
      "24           purchase_events_log   8167.338024\n",
      "25            total_sessions_log -10009.808819\n"
     ]
    }
   ],
   "source": [
    "def modelInfo(inputCols, pipelineGLRModel):\n",
    "    # Create a zipped list containing the coefficients and the data\n",
    "    modelCols = copy.deepcopy(inputCols)\n",
    "    modelCoeffs = list(pipelineGLRModel.stages[-1].coefficients)\n",
    "    modelCoeffs.insert(0,pipelineGLRModel.stages[-1].intercept)\n",
    "    modelCols.insert(0,\"intercept\")\n",
    "    modelZippedList = list(map(list, zip(modelCols, modelCoeffs)))\n",
    "\n",
    "    # Create the pandas DataFrame\n",
    "    modelDF = pd.DataFrame(modelZippedList, columns = ['Column name', 'Coefficient'])\n",
    "    return modelDF\n",
    "\n",
    "print(\"Model coefficients\")\n",
    "print(modelInfo(inputCols, pipelineGLRModel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate adjusted r2 (https://towardsdatascience.com/machine-learning-linear-regression-using-pyspark-9d5d5c772b42)\n",
    "def adj_r2(r2, inputCols, testDF):\n",
    "    n = testDF.count()\n",
    "    p = len(inputCols)\n",
    "    adjusted_r2 = 1-(((1-r2)*(n-1))/(n-p-1))\n",
    "    return adjusted_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|    T_total_spend|        prediction|\n",
      "+-----------------+------------------+\n",
      "|              0.0| 6884.991273173087|\n",
      "|              0.0| 4928.966481742349|\n",
      "|              0.0| 5191.554306991455|\n",
      "|              0.0|-2478.013688906618|\n",
      "|              0.0|-352.9844265453921|\n",
      "|              0.0| -3259.52817503016|\n",
      "|              0.0| 2352.099011215634|\n",
      "|              0.0| 8714.741227810528|\n",
      "|79118.00024795532|146048.51904571752|\n",
      "|663.1999969482422| 7569.375115314482|\n",
      "+-----------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "RMSE is 43389.1\n",
      "R^2 is 0.61596\n",
      "Adjusted R^2 is 0.61582\n"
     ]
    }
   ],
   "source": [
    "def getEvaluationMetrics(pipelineModel,outputCol,testDF,inputCols):\n",
    "    predDF = pipelineModel.transform(testDF)\n",
    "    predDF.select(outputCol, \"prediction\").show(10)\n",
    "\n",
    "    regressionEvaluator = RegressionEvaluator(\n",
    "    predictionCol=\"prediction\",\n",
    "    labelCol=outputCol,\n",
    "    metricName=\"rmse\")\n",
    "    rmse = regressionEvaluator.evaluate(predDF)\n",
    "\n",
    "    regressionEvaluator = RegressionEvaluator(\n",
    "    predictionCol=\"prediction\",\n",
    "    labelCol=outputCol,\n",
    "    metricName=\"r2\")\n",
    "    r2 = regressionEvaluator.evaluate(predDF)\n",
    "    \n",
    "    # Manually calculate Adjusted r2\n",
    "    adjusted_r2 = adj_r2(r2, inputCols, testDF)\n",
    "    \n",
    "    return rmse, r2, adjusted_r2\n",
    "\n",
    "evaluationMetrics = getEvaluationMetrics(pipelineGLRModel,\"T_total_spend\",testDF,inputCols)\n",
    "print(f\"RMSE is {evaluationMetrics[0]:.1f}\")\n",
    "print(f\"R^2 is {evaluationMetrics[1]:.5f}\")\n",
    "print(f\"Adjusted R^2 is {evaluationMetrics[2]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### While we want to do a tweedie model to test how a model built for zero-inflation works, it appears that this implementation is unable to create a mixture model using data such as ours. It generates an error 'Sum of weights cannot be zero' that we were unable to overcome - what we have found online indicates that this is an issue specifically with the pyspark implementation of this model. The model below uses data that we have coerced to the point that it is able to run, though the response variables are so modified as to be useless - we divided by 100, then added 1. Even then, it would not run unless we filtered to those with values originally <= 100. We provide it to prove only that the code works, tuning this model is useless, because it only works if all of the response values have had most of their variability scrubbed out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF2 = trainDF.filter(col('T_total_spend') <=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-evaluate tweedie variables (linkPower & varPower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For crossval to work, must define pipeline here\n",
    "# Select input columns for generalized linear regression\n",
    "vecAssembler = VectorAssembler(inputCols=inputCols, outputCol=\"features\")\n",
    "\n",
    "# create glr instance & select output col\n",
    "glr = GeneralizedLinearRegression(featuresCol = \"features\", labelCol = \"T_total_spend_pos\", family = \"tweedie\")\n",
    "\n",
    "pipeline = Pipeline(stages=[vecAssembler, glr])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(glr.variancePower, [1, 2]) \\\n",
    "    .addGrid(glr.linkPower, [2, 1.5]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator = pipeline,\n",
    "                         estimatorParamMaps=paramGrid,\n",
    "                         evaluator=RegressionEvaluator().setLabelCol(\"T_total_spend_pos\"),\n",
    "                         numFolds=4)\n",
    "# Run cross-validation, and choose best set of parameters.\n",
    "cvModel = crossval.fit(trainDF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model coefficients\n",
      "                     Column name   Coefficient\n",
      "0                      intercept  1.008924e+00\n",
      "1                    total_spend  1.068017e-06\n",
      "2                   total_events  4.217552e-03\n",
      "3                 total_sessions  2.033541e-03\n",
      "4             avg_session_length -1.258126e-07\n",
      "5   avg_interactions_per_session  3.255075e-05\n",
      "6   max_interactions_per_session -5.862676e-06\n",
      "7   purchase_pct_of_total_events  2.051163e-02\n",
      "8       view_pct_of_total_events -1.410037e-03\n",
      "9       cart_pct_of_total_events -7.652089e-03\n",
      "10     avg_purchases_per_session -2.249452e-03\n",
      "11                   cart_events -4.076877e-03\n",
      "12               purchase_events -5.219651e-03\n",
      "13                   view_events -4.207335e-03\n",
      "14        sessions_with_purchase  1.335609e-03\n",
      "15            sessions_with_cart -6.353159e-04\n",
      "16            sessions_with_view -2.012615e-03\n",
      "17     pct_sessions_end_purchase  4.937284e-03\n",
      "18         pct_sessions_end_cart  3.780298e-03\n",
      "19             sd_session_length  2.262800e-08\n",
      "20   sd_interactions_per_session -6.304232e-05\n",
      "21      sd_purchases_per_session  2.773276e-03\n",
      "22               total_spend_log -1.817128e-03\n",
      "23              total_events_log -3.445725e-04\n",
      "24           purchase_events_log  7.154860e-04\n",
      "25            total_sessions_log  3.224603e-04\n",
      "+-----------------+------------------+\n",
      "|T_total_spend_pos|        prediction|\n",
      "+-----------------+------------------+\n",
      "|              1.0|0.9989737196669914|\n",
      "|              1.0|0.9998455989149909|\n",
      "|              1.0|0.9993151424612843|\n",
      "|              1.0| 1.000921854868849|\n",
      "|              1.0|1.0009431537698663|\n",
      "|              1.0|1.0019200640484953|\n",
      "|              1.0|0.9998785890043451|\n",
      "|              1.0|0.9985122262183223|\n",
      "|792.1800024795532|1.5989146136714163|\n",
      "|7.631999969482422|1.0405306529572618|\n",
      "+-----------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "RMSE is 713.1\n",
      "R^2 is -0.03739\n",
      "Adjusted R^2 is -0.03776\n",
      "\n",
      "{Param(parent='GeneralizedLinearRegression_49579f1a8cd7', name='variancePower', doc='The power in the variance function of the Tweedie distribution which characterizes the relationship between the variance and mean of the distribution. Only applicable for the Tweedie family. Supported values: 0 and [1, Inf).'): 2.0, Param(parent='GeneralizedLinearRegression_49579f1a8cd7', name='linkPower', doc='The index in the power link function. Only applicable to the Tweedie family.'): 2.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best model coefficients\")\n",
    "print(modelInfo(inputCols, cvModel.bestModel))\n",
    "\n",
    "evaluationMetrics = getEvaluationMetrics(cvModel.bestModel,\"T_total_spend_pos\",testDF,inputCols)\n",
    "print(f\"RMSE is {evaluationMetrics[0]:.1f}\")\n",
    "print(f\"R^2 is {evaluationMetrics[1]:.5f}\")\n",
    "print(f\"Adjusted R^2 is {evaluationMetrics[2]:.5f}\")\n",
    "\n",
    "print()\n",
    "print(cvModel.getEstimatorParamMaps()[np.argmax(cvModel.avgMetrics)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshooting: https://stackoverflow.com/questions/49567921/pyspark-how-to-fit-a-glm-using-log-as-link-function-with-sum-of-weights-as-zero\n",
    "# Sum of weights cannot be 0 issue\n",
    "# Other suggestion is data is too long-tailed, try cutting it (treating more of spend as outlier)\n",
    "#trainDF = trainDF.withColumn(\"T_total_spend_small\", col(\"T_total_spend\")/1000) # Per one person's advice, try making the response values smaller \n",
    "\n",
    "# code that produces \"Sum of weights cannot be zero\" is here: https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/optim/WeightedLeastSquares.scala\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
