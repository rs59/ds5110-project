{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File 07 - Random Forest\n",
    "\n",
    "##### Group 12:\n",
    "\n",
    "##### Hannah Schmuckler, mmc4cv\n",
    "\n",
    "##### Rob Schwartz, res7cd\n",
    "\n",
    "In this file, we create an ML pipeline based on the output from File 02 (Feature creation).\n",
    "\n",
    "The files needed are `/processed_data/train.parquet` and `/processed_data/test.parquet`.\n",
    "\n",
    "We create a random forest model and fine-tune. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Spark session\n",
    "\n",
    "We can specify more options in the SparkSession creator, but currently the options are at the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 470 ms, sys: 365 ms, total: 836 ms\n",
      "Wall time: 8.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import log\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in dataframes for train and test sets\n",
    "\n",
    "This data should have been previously generated: we can find it in the `processed_data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.95 ms, sys: 2.04 ms, total: 3.99 ms\n",
      "Wall time: 4.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainDF = spark.read.parquet(\"./processed_data/train.parquet\")\n",
    "testDF = spark.read.parquet(\"./processed_data/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Spark ML pipeline training for random forest\n",
    "\n",
    "We create the function `generatePipeline(inputCols, outputCol)`, Then, we train the pipeline using this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def generatePipeline(inputCols, outputCol):\n",
    "    \n",
    "    # Select input columns for random forest regression\n",
    "    vecAssembler = VectorAssembler(inputCols=inputCols, outputCol=\"features\")\n",
    "\n",
    "    # Select output column for random forest regression\n",
    "    rf = RandomForestRegressor(featuresCol=\"features\", labelCol=outputCol, seed = 42)#, numTrees=5, maxDepth=5)\n",
    "    \n",
    "    pipeline = Pipeline(stages=[vecAssembler, rf])\n",
    "    return pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate adjusted r2 (https://towardsdatascience.com/machine-learning-linear-regression-using-pyspark-9d5d5c772b42)\n",
    "def adj_r2(r2, inputCols, testDF):\n",
    "    n = testDF.count()\n",
    "    p = len(inputCols)\n",
    "    adjusted_r2 = 1-(((1-r2)*(n-1))/(n-p-1))\n",
    "    return adjusted_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEvaluationMetrics(pipelineMode,outputCol,testDF,inputCols):\n",
    "    predDF = pipelineModel.transform(testDF)\n",
    "    predDF.select(outputCol, \"prediction\").show(10)\n",
    "    \n",
    "    regressionEvaluator = RegressionEvaluator(\n",
    "    predictionCol=\"prediction\",\n",
    "    labelCol=outputCol,\n",
    "    metricName=\"rmse\")\n",
    "    rmse = regressionEvaluator.evaluate(predDF)\n",
    "\n",
    "    regressionEvaluator = RegressionEvaluator(\n",
    "    predictionCol=\"prediction\",\n",
    "    labelCol=outputCol,\n",
    "    metricName=\"r2\")\n",
    "    r2 = regressionEvaluator.evaluate(predDF)\n",
    "    \n",
    "    # Manually calculate Adjusted r2\n",
    "    adjusted_r2 = adj_r2(r2, inputCols, testDF)\n",
    "    \n",
    "    return rmse, r2, adjusted_r2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelInfo(inputCols, pipelineModel):\n",
    "    modelCols = pipelineModel.stages[-2].getInputCols()\n",
    "    \n",
    "    feature_importance = pipelineModel.stages[-1].featureImportances\n",
    "    \n",
    "    return pd.DataFrame(list(zip(modelCols, feature_importance)), columns = ['Column name', 'Importance']).sort_values(by=\"Importance\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** All inputs + normal output **\n",
      "                     Column name  Importance\n",
      "20               total_spend_log    0.171629\n",
      "15        sessions_with_purchase    0.162941\n",
      "0                    total_spend    0.138880\n",
      "16            sessions_with_cart    0.091195\n",
      "10      cart_pct_of_total_events    0.048967\n",
      "22           purchase_events_log    0.047236\n",
      "3             avg_session_length    0.046953\n",
      "14               purchase_events    0.038467\n",
      "6    sd_interactions_per_session    0.032558\n",
      "24        avg_session_length_log    0.031682\n",
      "19         pct_sessions_end_cart    0.028497\n",
      "5   avg_interactions_per_session    0.028233\n",
      "9       view_pct_of_total_events    0.023288\n",
      "4              sd_session_length    0.022964\n",
      "8   purchase_pct_of_total_events    0.021720\n",
      "7   max_interactions_per_session    0.018826\n",
      "2                 total_sessions    0.008926\n",
      "12      sd_purchases_per_session    0.008833\n",
      "18     pct_sessions_end_purchase    0.007736\n",
      "11     avg_purchases_per_session    0.006570\n",
      "23            total_sessions_log    0.005602\n",
      "13                   cart_events    0.004001\n",
      "1                   total_events    0.001816\n",
      "17            sessions_with_view    0.001508\n",
      "21              total_events_log    0.000970\n",
      "+------------------+------------------+\n",
      "|     T_total_spend|        prediction|\n",
      "+------------------+------------------+\n",
      "|               0.0|121.81735882681588|\n",
      "|               0.0|130.41334391486657|\n",
      "|               0.0| 119.8557596227511|\n",
      "|               0.0|121.81735882681588|\n",
      "|               0.0| 183.6287220930429|\n",
      "|               0.0|181.35239012944928|\n",
      "| 312.4200134277344|172.58042669308185|\n",
      "|  6009.78010559082| 510.8524211935088|\n",
      "|1627.1900024414062| 187.3897525636572|\n",
      "|1393.8800354003906|170.89223428852364|\n",
      "+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "RMSE is 2267.0\n",
      "R^2 is 0.15360\n",
      "Adjusted R^2 is 0.15330\n"
     ]
    }
   ],
   "source": [
    "print(\"** All inputs + normal output **\")\n",
    "inputCols = [\"total_spend\", \"total_events\", \"total_sessions\", \"avg_session_length\", \"sd_session_length\", \"avg_interactions_per_session\", \n",
    "             \"sd_interactions_per_session\", \"max_interactions_per_session\", \"purchase_pct_of_total_events\", \"view_pct_of_total_events\",\n",
    "             \"cart_pct_of_total_events\", \"avg_purchases_per_session\", \"sd_purchases_per_session\", \"cart_events\", \"purchase_events\", \n",
    "             \"sessions_with_purchase\", \"sessions_with_cart\", \"sessions_with_view\", \"pct_sessions_end_purchase\", \n",
    "             \"pct_sessions_end_cart\", \"total_spend_log\", \"total_events_log\", \"purchase_events_log\", \"total_sessions_log\",\n",
    "             \"avg_session_length_log\"]\n",
    "outputCol = \"T_total_spend\" #Performance is about .106 with log output. \n",
    "\n",
    "pipeline = generatePipeline(inputCols, outputCol)\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "\n",
    "print(modelInfo(inputCols, pipelineModel))\n",
    "\n",
    "evaluationMetrics = getEvaluationMetrics(pipelineModel,outputCol,testDF, inputCols)\n",
    "print(f\"RMSE is {evaluationMetrics[0]:.1f}\")\n",
    "print(f\"R^2 is {evaluationMetrics[1]:.5f}\")\n",
    "print(f\"Adjusted R^2 is {evaluationMetrics[2]:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Champion features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Reduced inputs, normal output **\n",
      "                     Column name  Importance\n",
      "18               total_spend_log    0.140809\n",
      "0                    total_spend    0.136926\n",
      "14        sessions_with_purchase    0.120653\n",
      "15            sessions_with_cart    0.105322\n",
      "19           purchase_events_log    0.087959\n",
      "13               purchase_events    0.057052\n",
      "2             avg_session_length    0.044503\n",
      "9       cart_pct_of_total_events    0.043786\n",
      "7   purchase_pct_of_total_events    0.039523\n",
      "3              sd_session_length    0.034424\n",
      "21        avg_session_length_log    0.033345\n",
      "8       view_pct_of_total_events    0.028719\n",
      "4   avg_interactions_per_session    0.024396\n",
      "6   max_interactions_per_session    0.016200\n",
      "11      sd_purchases_per_session    0.015675\n",
      "5    sd_interactions_per_session    0.013474\n",
      "17         pct_sessions_end_cart    0.011887\n",
      "12                   cart_events    0.011849\n",
      "10     avg_purchases_per_session    0.010792\n",
      "20            total_sessions_log    0.008766\n",
      "16     pct_sessions_end_purchase    0.007253\n",
      "1                 total_sessions    0.006689\n",
      "+------------------+------------------+\n",
      "|     T_total_spend|        prediction|\n",
      "+------------------+------------------+\n",
      "|               0.0|   128.96718512969|\n",
      "|               0.0|130.50915088811848|\n",
      "|               0.0|126.93085988451864|\n",
      "|               0.0|130.50915088811848|\n",
      "|               0.0| 164.1481583696438|\n",
      "|               0.0|209.09686460782777|\n",
      "| 312.4200134277344|165.69012412807226|\n",
      "|  6009.78010559082| 529.9442741093114|\n",
      "|1627.1900024414062|203.73672310971511|\n",
      "|1393.8800354003906|155.98333182100924|\n",
      "+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "RMSE is 2260.1\n",
      "R^2 is 0.15874\n",
      "Adjusted R^2 is 0.15848\n"
     ]
    }
   ],
   "source": [
    "print(\"** Reduced inputs, normal output **\")\n",
    "inputCols = [\"total_spend\", \"total_sessions\", \"avg_session_length\", \"sd_session_length\", \"avg_interactions_per_session\", \n",
    "             \"sd_interactions_per_session\", \"max_interactions_per_session\", \"purchase_pct_of_total_events\", \"view_pct_of_total_events\",\n",
    "             \"cart_pct_of_total_events\", \"avg_purchases_per_session\", \"sd_purchases_per_session\", \"cart_events\", \"purchase_events\", \n",
    "             \"sessions_with_purchase\", \"sessions_with_cart\", \"pct_sessions_end_purchase\", \n",
    "             \"pct_sessions_end_cart\", \"total_spend_log\", \"purchase_events_log\", \"total_sessions_log\",\n",
    "             \"avg_session_length_log\"]\n",
    "\n",
    "outputCol = \"T_total_spend\"\n",
    "\n",
    "pipeline = generatePipeline(inputCols, outputCol)\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "\n",
    "print(modelInfo(inputCols, pipelineModel))\n",
    "\n",
    "evaluationMetrics = getEvaluationMetrics(pipelineModel,outputCol,testDF, inputCols)\n",
    "print(f\"RMSE is {evaluationMetrics[0]:.1f}\")\n",
    "print(f\"R^2 is {evaluationMetrics[1]:.5f}\")\n",
    "print(f\"Adjusted R^2 is {evaluationMetrics[2]:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test out different combinations of hyperparameters on best features model. We did this in small batches because the random forests take so much memory to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.63 ms, sys: 4.72 ms, total: 14.4 ms\n",
      "Wall time: 2.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inputCols = [\"total_spend\", \"total_sessions\", \"avg_session_length\", \"sd_session_length\", \"avg_interactions_per_session\", \n",
    "             \"sd_interactions_per_session\", \"max_interactions_per_session\", \"purchase_pct_of_total_events\", \"view_pct_of_total_events\",\n",
    "             \"cart_pct_of_total_events\", \"avg_purchases_per_session\", \"sd_purchases_per_session\", \"cart_events\", \"purchase_events\", \n",
    "             \"sessions_with_purchase\", \"sessions_with_cart\", \"pct_sessions_end_purchase\", \n",
    "             \"pct_sessions_end_cart\", \"total_spend_log\", \"purchase_events_log\", \"total_sessions_log\",\n",
    "             \"avg_session_length_log\"]\n",
    "\n",
    "outputCol = \"T_total_spend\"\n",
    "\n",
    "pipeline = generatePipeline(inputCols, outputCol)\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=inputCols, outputCol=\"features\")\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=outputCol)\n",
    "\n",
    "pipeline = Pipeline(stages=[vecAssembler, rf])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [5, 10, 20]) \\\n",
    "    .addGrid(rf.maxDepth, [4, 5, 10]) \\\n",
    "    .addGrid(rf.featureSubsetStrategy, ['sqrt', 'log2', 'onethird']) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator().setLabelCol(\"T_total_spend\"),\n",
    "                          numFolds=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.76 s, sys: 771 ms, total: 4.53 s\n",
      "Wall time: 4min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cvModel = crossval.setParallelism(8).fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|     T_total_spend|        prediction|\n",
      "+------------------+------------------+\n",
      "|               0.0|   128.96718512969|\n",
      "|               0.0|130.50915088811848|\n",
      "|               0.0|126.93085988451864|\n",
      "|               0.0|130.50915088811848|\n",
      "|               0.0| 164.1481583696438|\n",
      "|               0.0|209.09686460782777|\n",
      "| 312.4200134277344|165.69012412807226|\n",
      "|  6009.78010559082| 529.9442741093114|\n",
      "|1627.1900024414062|203.73672310971511|\n",
      "|1393.8800354003906|155.98333182100924|\n",
      "+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "RMSE is 2260.1\n",
      "R^2 is 0.15874\n",
      "Adjusted R^2 is 0.15848\n",
      "\n",
      "{Param(parent='RandomForestRegressor_fc1ab56f1913', name='numTrees', doc='Number of trees to train (>= 1).'): 5, Param(parent='RandomForestRegressor_fc1ab56f1913', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10, Param(parent='RandomForestRegressor_fc1ab56f1913', name='featureSubsetStrategy', doc=\"The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto'\"): 'sqrt'}\n",
      "\n",
      "[2346.9856767375904, 2346.9856767375904, 2340.615023976971, 2356.8657443628354, 2356.8657443628354, 2357.843402071946, 2495.390258318112, 2495.390258318112, 2485.0536969309924, 2324.782262091144, 2324.782262091144, 2324.381752348262, 2318.0794585791155, 2318.0794585791155, 2331.0662694092075, 2382.749573442989, 2382.749573442989, 2405.0862361990194, 2313.2187918254576, 2313.2187918254576, 2325.5902506951675, 2315.295246205385, 2315.295246205385, 2328.9635901897796, 2348.2149185805415, 2348.2149185805415, 2377.0907636004877]\n"
     ]
    }
   ],
   "source": [
    "evaluationMetrics = getEvaluationMetrics(cvModel.bestModel,\"T_total_spend\",testDF,inputCols)\n",
    "print(f\"RMSE is {evaluationMetrics[0]:.1f}\")\n",
    "print(f\"R^2 is {evaluationMetrics[1]:.5f}\")\n",
    "print(f\"Adjusted R^2 is {evaluationMetrics[2]:.5f}\")\n",
    "\n",
    "print()\n",
    "print(cvModel.getEstimatorParamMaps()[np.argmax(cvModel.avgMetrics)])\n",
    "print()\n",
    "print(cvModel.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
