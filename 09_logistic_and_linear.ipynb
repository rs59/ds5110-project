{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File 10 - Logistic + Linear Regression\n",
    "\n",
    "In this file, we create a small ML pipeline based on the output from File 02 (Feature creation).\n",
    "\n",
    "We create a linear regression model, tune it, then compare it to a linear regression model created with downsampled data to see how performance compares. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Spark session\n",
    "\n",
    "We can specify more options in the SparkSession creator, but currently the options are at the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 949 ms, sys: 743 ms, total: 1.69 s\n",
      "Wall time: 6.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, LinearRegressionModel, LinearRegressionSummary\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import log\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from handyspark import *\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in dataframes for train and test sets\n",
    "\n",
    "This data should have been previously generated: we can find it in the `processed_data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.12 ms, sys: 2.18 ms, total: 4.3 ms\n",
      "Wall time: 3.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainDF = spark.read.parquet(\"./processed_data/train.parquet\")\n",
    "testDF = spark.read.parquet(\"./processed_data/test.parquet\")\n",
    "# trainDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label trainDF with 0 if they did not spend in m2 and 1 if they did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.withColumn('logistic_T_total_spend', when(trainDF.T_total_spend == 0, 0)\n",
    "                                                       .otherwise(1))\n",
    "testDF = testDF.withColumn('logistic_T_total_spend', when(testDF.T_total_spend == 0, 0)\n",
    "                                                       .otherwise(1))\n",
    "\n",
    "trainDF = trainDF.withColumn('logistic_T_total_spend', col(\"logistic_T_total_spend\").cast(DoubleType()))\n",
    "testDF = testDF.withColumn('logistic_T_total_spend', col(\"logistic_T_total_spend\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Spark ML pipeline training for linear regression\n",
    "\n",
    "Here we decide which input columns should be used in order to create our training pipeline. To implement this step, we create the function `generatePipeline(inputCols, outputCol)`. Then, we train the pipeline using this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 7.63 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def generateLogisticPipeline(inputCols, outputCol, threshold):\n",
    "    # Select input columns for linear regression\n",
    "    vecAssembler = VectorAssembler(inputCols=inputCols, outputCol=\"features\")\n",
    "\n",
    "    # Select output column for linear regression\n",
    "    logR = LogisticRegression(featuresCol=\"features\", labelCol=outputCol, rawPredictionCol = \"Logistic_Probabilities\", predictionCol = \"Logistic_Predictions\", threshold = threshold)\n",
    "\n",
    "    # The following lines (pipeline creation and fitting) replace these two commented-out lines.\n",
    "    # vecTrainDF = vecAssembler.transform(trainDF)\n",
    " \n",
    "    pipeline = Pipeline(stages=[vecAssembler, logR])\n",
    "    return pipeline\n",
    "    \n",
    "#pipeline = generateLogisticPipeline(inputCols, \"logistic_T_total_spend\")\n",
    "#pipelineModel = pipeline.fit(trainDF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the model information\n",
    "\n",
    "Print out the model coefficients and view the pValues, RMSE and R^2. We define the functions `modelInfo(inputCols, pipelineModel)` and `getEvaluationMetrics(pipelineModel,outputCol,testDF)` to report this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelInfo(inputCols, pipelineModel):\n",
    "    # Create a zipped list containing the coefficients and the data\n",
    "    modelCols = copy.deepcopy(inputCols)\n",
    "    modelCoeffs = list(pipelineModel.stages[-1].coefficients)\n",
    "    modelCoeffs.insert(0,pipelineModel.stages[-1].intercept)\n",
    "    modelCols.insert(0,\"intercept\")\n",
    "    modelZippedList = list(map(list, zip(modelCols, modelCoeffs)))\n",
    "    \n",
    "    # Add in the p-values\n",
    "    #pvals = pipelineModel.stages[-1].summary.pValues\n",
    "    \n",
    "    # Create the pandas DataFrame\n",
    "    modelDF = pd.DataFrame(modelZippedList, columns = ['Column name', 'Coefficient'])\n",
    "    #modelDF['pValues'] = pvals\n",
    "    return modelDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate adjusted r2 (https://towardsdatascience.com/machine-learning-linear-regression-using-pyspark-9d5d5c772b42)\n",
    "# This function will allow us to calculate the adjusted r-square value when we do PCA later. The default r-square function does not take into account k. \n",
    "def adj_r2(r2, inputCols, testDF, k = 0):\n",
    "    n = testDF.count()\n",
    "    if k == 0:\n",
    "        p = len(inputCols)\n",
    "    else: \n",
    "        p = len(inputCols) + k - 1\n",
    "    \n",
    "    adjusted_r2 = 1-(((1-r2)*(n-1))/(n-p-1))\n",
    "    return adjusted_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/binary-classifier-evaluation-made-easy-with-handyspark-3b1e69c12b4f\n",
    "def getLogisticEvaluationMetrics(pipelineModel,outputCol,testDF,inputCols):\n",
    "    predDF = pipelineModel.transform(testDF)\n",
    "    preds_only = predDF.select(outputCol, \"Logistic_Predictions\")\n",
    "    preds_only.show(5)\n",
    "    \n",
    "    preds_only = preds_only.rdd\n",
    "    \n",
    "    metrics = MulticlassMetrics(preds_only)\n",
    "    print(metrics.confusionMatrix().toArray())\n",
    "    print(\"Accuracy: \" + str(metrics.accuracy))\n",
    "\n",
    "    return predDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Column name  Coefficient\n",
      "0                      intercept    -1.184833\n",
      "1                    total_spend     0.000047\n",
      "2                   total_events     0.879037\n",
      "3                purchase_events    -0.496094\n",
      "4                 total_sessions     0.289156\n",
      "5             avg_session_length     0.000017\n",
      "6   avg_interactions_per_session    -0.030066\n",
      "7   max_interactions_per_session     0.010927\n",
      "8   purchase_pct_of_total_events    -1.052250\n",
      "9       view_pct_of_total_events    -1.453185\n",
      "10      cart_pct_of_total_events    -1.826852\n",
      "11     avg_purchases_per_session    -0.288964\n",
      "12                   cart_events    -0.882949\n",
      "13               purchase_events    -0.496094\n",
      "14                   view_events    -0.877355\n",
      "15        sessions_with_purchase     0.196496\n",
      "16            sessions_with_cart    -0.014244\n",
      "17            sessions_with_view    -0.278276\n",
      "18     pct_sessions_end_purchase     0.715227\n",
      "19         pct_sessions_end_cart     0.188995\n",
      "20             sd_session_length    -0.000005\n",
      "21   sd_interactions_per_session    -0.017249\n",
      "22      sd_purchases_per_session     0.344958\n",
      "+----------------------+--------------------+\n",
      "|logistic_T_total_spend|Logistic_Predictions|\n",
      "+----------------------+--------------------+\n",
      "|                   0.0|                 0.0|\n",
      "|                   0.0|                 0.0|\n",
      "|                   0.0|                 0.0|\n",
      "|                   0.0|                 0.0|\n",
      "|                   0.0|                 0.0|\n",
      "+----------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[[52051.  5751.]\n",
      " [   55. 11532.]]\n",
      "Accuracy: 0.9163267953133782\n"
     ]
    }
   ],
   "source": [
    "inputCols = [\"total_spend\",\"total_events\",\"purchase_events\", \"total_sessions\", \"avg_session_length\", \"avg_interactions_per_session\", \"max_interactions_per_session\",\n",
    "             \"purchase_pct_of_total_events\", \"view_pct_of_total_events\", \"cart_pct_of_total_events\",\"avg_purchases_per_session\", \"cart_events\", \"purchase_events\",\n",
    "             \"view_events\", \"sessions_with_purchase\", \"sessions_with_cart\",\"sessions_with_view\", \"pct_sessions_end_purchase\", \"pct_sessions_end_cart\", 'sd_session_length', \n",
    "             'sd_interactions_per_session', 'sd_purchases_per_session']\n",
    "outputCol= \"logistic_T_total_spend\"\n",
    "\n",
    "pipeline = generateLogisticPipeline(inputCols, outputCol, .6)\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "\n",
    "print(modelInfo(inputCols, pipelineModel))\n",
    "\n",
    "logistic_predictions = getLogisticEvaluationMetrics(pipelineModel,outputCol,testDF, inputCols)\n",
    "#print(f\"RMSE is {evaluationMetrics[0]:.1f}\")\n",
    "#print(f\"R^2 is {evaluationMetrics[1]:.5f}\")\n",
    "#print(f\"Adjusted R^2 is {evaluationMetrics[2]:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threshold of .6 maximizes accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we'll train a linear regression model using only the true positive training data, and generate a regression prediction for all test data that had a prediction of positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_2_train = trainDF.filter(col(\"logistic_T_total_spend\") == 1)\n",
    "part_2_test = logistic_predictions.filter(col(\"Logistic_Predictions\") == 1).withColumnRenamed('probability', \"ProbabilityLogistic\")\n",
    "\n",
    "part_1_test = logistic_predictions.filter(col(\"Logistic_Predictions\") == 0).withColumnRenamed('probability', \"ProbabilityLogistic\") \\\n",
    "    .withColumn(\"\", lit(None)) ## For overall accuracy metrics later, will be joined with output of part_2_test's regression predictions\n",
    "part_1_test = part_1_test.withColumn(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add empty columns to pad it to union later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-----------------+------------+--------------+------------------+------------------+----------------------------+---------------------------+----------------------------+----------------------------+------------------------+------------------------+-------------------------+------------------------+-----------+---------------+-----------+----------------------+------------------+------------------+-------------------------+---------------------+-----------------+----------------+-------------------+------------------+-----------------+--------------------+--------------------+--------------------+--------------------+----------------------+\n",
      "|  user_id|     T_total_spend|      total_spend|total_events|total_sessions|avg_session_length| sd_session_length|avg_interactions_per_session|sd_interactions_per_session|max_interactions_per_session|purchase_pct_of_total_events|view_pct_of_total_events|cart_pct_of_total_events|avg_purchases_per_session|sd_purchases_per_session|cart_events|purchase_events|view_events|sessions_with_purchase|sessions_with_cart|sessions_with_view|pct_sessions_end_purchase|pct_sessions_end_cart|  total_spend_log|total_events_log|purchase_events_log|total_sessions_log|T_total_spend_log|     pca_purchases10|     pca_purchases20|     pca_purchases50|    pca_purchases100|logistic_T_total_spend|\n",
      "+---------+------------------+-----------------+------------+--------------+------------------+------------------+----------------------------+---------------------------+----------------------------+----------------------------+------------------------+------------------------+-------------------------+------------------------+-----------+---------------+-----------+----------------------+------------------+------------------+-------------------------+---------------------+-----------------+----------------+-------------------+------------------+-----------------+--------------------+--------------------+--------------------+--------------------+----------------------+\n",
      "|512424146|11270.879859924316|526.6399841308594|         288|             7| 260.7142857142857|262.34564622885983|           5.142857142857143|         2.4784787961282104|                           8|         0.08333333333333333|      0.7777777777777778|      0.1388888888888889|      0.42857142857142855|      0.5345224838248488|          5|              3|         28|                     3|                 4|                 7|      0.42857142857142855|  0.14285714285714285|6.266519071855391|5.66296395235214| 1.0989455664582302|1.9460529959950605|9.329977763688856|[-0.0221225462344...|[-0.0221225462344...|[-0.0221225462344...|[-0.0221225462344...|                   1.0|\n",
      "+---------+------------------+-----------------+------------+--------------+------------------+------------------+----------------------------+---------------------------+----------------------------+----------------------------+------------------------+------------------------+-------------------------+------------------------+-----------+---------------+-----------+----------------------+------------------+------------------+-------------------------+---------------------+-----------------+----------------+-------------------+------------------+-----------------+--------------------+--------------------+--------------------+--------------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "part_2_train.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+----------------+------------+--------------+------------------+-----------------+----------------------------+---------------------------+----------------------------+----------------------------+------------------------+------------------------+-------------------------+------------------------+-----------+---------------+-----------+----------------------+------------------+------------------+-------------------------+---------------------+-----------------+-----------------+-------------------+------------------+------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+----------------------+--------------------+--------------------+\n",
      "|  user_id|    T_total_spend|     total_spend|total_events|total_sessions|avg_session_length|sd_session_length|avg_interactions_per_session|sd_interactions_per_session|max_interactions_per_session|purchase_pct_of_total_events|view_pct_of_total_events|cart_pct_of_total_events|avg_purchases_per_session|sd_purchases_per_session|cart_events|purchase_events|view_events|sessions_with_purchase|sessions_with_cart|sessions_with_view|pct_sessions_end_purchase|pct_sessions_end_cart|  total_spend_log| total_events_log|purchase_events_log|total_sessions_log| T_total_spend_log|     pca_purchases10|     pca_purchases20|     pca_purchases50|    pca_purchases100|logistic_T_total_spend|            features|Logistic_Probabilities| ProbabilityLogistic|Logistic_Predictions|\n",
      "+---------+-----------------+----------------+------------+--------------+------------------+-----------------+----------------------------+---------------------------+----------------------------+----------------------------+------------------------+------------------------+-------------------------+------------------------+-----------+---------------+-----------+----------------------+------------------+------------------+-------------------------+---------------------+-----------------+-----------------+-------------------+------------------+------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+----------------------+--------------------+--------------------+\n",
      "|512763956|79118.00024795532|19093.1201171875|         416|            17|59.294117647058826|63.99977022017575|          3.0588235294117645|         2.1929029913363163|                           8|         0.19230769230769232|      0.5576923076923077|                    0.25|       0.5882352941176471|      0.5072996561958923|         13|             10|         29|                    10|                11|                14|       0.5882352941176471|  0.11764705882352941|9.857083398232207|6.030687664104528| 2.3026850879943788| 2.833272165855592|11.278695703691795|[-0.0507863601337...|[-0.0507863601337...|[-0.0507863601337...|[-0.0507863601337...|                   1.0|[19093.1201171875...|  [-320.31189911851...|[7.76800908592991...|                 1.0|\n",
      "+---------+-----------------+----------------+------------+--------------+------------------+-----------------+----------------------------+---------------------------+----------------------------+----------------------------+------------------------+------------------------+-------------------------+------------------------+-----------+---------------+-----------+----------------------+------------------+------------------+-------------------------+---------------------+-----------------+-----------------+-------------------+------------------+------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+----------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "part_2_test.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------------+------------+--------------+------------------+------------------+----------------------------+---------------------------+----------------------------+----------------------------+------------------------+------------------------+-------------------------+------------------------+-----------+---------------+-----------+----------------------+------------------+------------------+-------------------------+---------------------+-----------------+------------------+--------------------+------------------+------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+----------------------+--------------------+--------------------+----+\n",
      "|  user_id|T_total_spend|      total_spend|total_events|total_sessions|avg_session_length| sd_session_length|avg_interactions_per_session|sd_interactions_per_session|max_interactions_per_session|purchase_pct_of_total_events|view_pct_of_total_events|cart_pct_of_total_events|avg_purchases_per_session|sd_purchases_per_session|cart_events|purchase_events|view_events|sessions_with_purchase|sessions_with_cart|sessions_with_view|pct_sessions_end_purchase|pct_sessions_end_cart|  total_spend_log|  total_events_log| purchase_events_log|total_sessions_log| T_total_spend_log|     pca_purchases10|     pca_purchases20|     pca_purchases50|    pca_purchases100|logistic_T_total_spend|            features|Logistic_Probabilities| ProbabilityLogistic|Logistic_Predictions|    |\n",
      "+---------+-------------+-----------------+------------+--------------+------------------+------------------+----------------------------+---------------------------+----------------------------+----------------------------+------------------------+------------------------+-------------------------+------------------------+-----------+---------------+-----------+----------------------+------------------+------------------+-------------------------+---------------------+-----------------+------------------+--------------------+------------------+------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+----------------------+--------------------+--------------------+----+\n",
      "|476330966|          0.0|411.0799865722656|          50|             4|           1233.75|2180.7546667763127|                        12.5|         16.522711641858304|                          37|                        0.02|                    0.94|                    0.04|                     0.25|                     0.5|          2|              1|         47|                     1|                 1|                 4|                     0.25|                  0.0|6.018790242705986|3.9120430052281487|9.995003330834232E-4|1.3865443298750981|-6.907755278982137|[-0.0092499310279...|[-0.0092499310279...|[-0.0092499310279...|[-0.0092499310279...|                   0.0|[411.079986572265...|  [2.40870142016819...|[0.91748842822892...|                 0.0|null|\n",
      "+---------+-------------+-----------------+------------+--------------+------------------+------------------+----------------------------+---------------------------+----------------------------+----------------------------+------------------------+------------------------+-------------------------+------------------------+-----------+---------------+-----------+----------------------+------------------+------------------+-------------------------+---------------------+-----------------+------------------+--------------------+------------------+------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+----------------------+--------------------+--------------------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "part_1_test.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69465\n",
      "11587\n"
     ]
    }
   ],
   "source": [
    "print(part_2_train.count())\n",
    "print(part_2_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 ms, sys: 2.98 ms, total: 20.8 ms\n",
      "Wall time: 1.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def generateLinearPipeline(inputCols, outputCol):\n",
    "    # Select input columns for linear regression\n",
    "    vecAssembler = VectorAssembler(inputCols=inputCols, outputCol=\"features2\")\n",
    "\n",
    "    # Select output column for linear regression\n",
    "    lr = LinearRegression(featuresCol=\"features2\", labelCol=outputCol)\n",
    "\n",
    "    # The following lines (pipeline creation and fitting) replace these two commented-out lines.\n",
    "    # vecTrainDF = vecAssembler.transform(trainDF)\n",
    " \n",
    "    pipeline = Pipeline(stages=[vecAssembler, lr])\n",
    "    return pipeline\n",
    "    \n",
    "pipeline = generateLinearPipeline(inputCols, \"T_total_spend\")\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinearEvaluationMetrics(linearPipelineModel,outputCol,testDF,inputCols):\n",
    "    new_preds = pipelineModel.transform(testDF)\n",
    "    #new_preds.show(1)\n",
    "    #part_1_test.show(1)\n",
    "    \n",
    "    predDF = new_preds.unionByName(part_1_test, allowMissingColumns=True)\n",
    "    #predDF.show(1)\n",
    "\n",
    "    predDF = predDF.withColumn(\"FinalPredictions\", when(col(\"Logistic_Predictions\") == 0, 0.0)\n",
    "                               .otherwise(col(\"prediction\")))\n",
    "    #predDF.select(outputCol, \"prediction\").show(10)\n",
    "\n",
    "    predDF.select(outputCol, \"FinalPredictions\").show()\n",
    "    \n",
    "    regressionEvaluator = RegressionEvaluator(\n",
    "    predictionCol=\"FinalPredictions\",\n",
    "    labelCol=outputCol,\n",
    "    metricName=\"rmse\")\n",
    "    rmse = regressionEvaluator.evaluate(predDF)\n",
    "\n",
    "    regressionEvaluator = RegressionEvaluator(\n",
    "    predictionCol=\"FinalPredictions\",\n",
    "    labelCol=outputCol,\n",
    "    metricName=\"r2\")\n",
    "    r2 = regressionEvaluator.evaluate(predDF)\n",
    "      \n",
    "    # Manually calculate Adjusted r2\n",
    "    adjusted_r2 = adj_r2(r2, inputCols, predDF)\n",
    "    \n",
    "    return rmse, r2, adjusted_r2, predDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** All original inputs, original output **\n",
      "Model coefficients\n",
      "                     Column name   Coefficient\n",
      "0                      intercept     42.791957\n",
      "1                    total_spend      4.715632\n",
      "2                   total_events    198.240063\n",
      "3                purchase_events  -4257.149567\n",
      "4                 total_sessions    340.222601\n",
      "5             avg_session_length      0.008373\n",
      "6   avg_interactions_per_session   -331.880827\n",
      "7   max_interactions_per_session    329.062660\n",
      "8   purchase_pct_of_total_events -47159.037863\n",
      "9       view_pct_of_total_events  14460.859569\n",
      "10      cart_pct_of_total_events -10157.956380\n",
      "11     avg_purchases_per_session  -3629.363791\n",
      "12                   cart_events    443.925907\n",
      "13                   view_events   -139.962083\n",
      "14        sessions_with_purchase  -2455.489738\n",
      "15            sessions_with_cart   1901.689269\n",
      "16            sessions_with_view    134.391390\n",
      "17     pct_sessions_end_purchase   7923.069760\n",
      "18         pct_sessions_end_cart  -6030.981149\n",
      "19             sd_session_length      0.040548\n",
      "20   sd_interactions_per_session   -179.416613\n",
      "21      sd_purchases_per_session    279.406909\n",
      "+------------------+------------------+\n",
      "|     T_total_spend|  FinalPredictions|\n",
      "+------------------+------------------+\n",
      "| 79118.00024795532| 135357.4511915077|\n",
      "| 663.1999969482422| 18696.28443091938|\n",
      "| 69578.50036621094| 76938.52818841444|\n",
      "| 17655.68035888672| 39607.97264406658|\n",
      "|  27249.8092918396| 55936.82456594552|\n",
      "| 12513.76968383789|25883.805197867183|\n",
      "|15674.449337005615| 13124.05052358283|\n",
      "|125526.95779418945| 43214.38409364918|\n",
      "| 87714.00268554688| 67515.04348363155|\n",
      "|11294.640197753906| 71157.57326597357|\n",
      "|6427.9998779296875| 5619.088771070729|\n",
      "|  76407.3623046875|148219.95501268742|\n",
      "| 206255.4348449707|101399.07780313709|\n",
      "|3751.1998901367188| 7902.097682269434|\n",
      "|11172.100067138672| 13405.99550645879|\n",
      "|  4806.56005859375| 7376.627934222832|\n",
      "| 1614.829963684082|16570.498954857092|\n",
      "|10626.119934082031| 4109.058680884385|\n",
      "|14093.660060882568|25605.943967083218|\n",
      "|7709.0997314453125|255.42382480741588|\n",
      "+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE is 43093.0\n",
      "R^2 is 0.62118\n",
      "Adjusted R^2 is 0.62107\n"
     ]
    }
   ],
   "source": [
    "print(\"** All original inputs, original output **\")\n",
    "inputCols = [\"total_spend\",\"total_events\",\"purchase_events\", \"total_sessions\", \"avg_session_length\", \"avg_interactions_per_session\", \"max_interactions_per_session\",\n",
    "             \"purchase_pct_of_total_events\", \"view_pct_of_total_events\", \"cart_pct_of_total_events\",\"avg_purchases_per_session\", \"cart_events\", \"view_events\", \n",
    "             \"sessions_with_purchase\", \"sessions_with_cart\",\"sessions_with_view\", \"pct_sessions_end_purchase\", \"pct_sessions_end_cart\", 'sd_session_length', \n",
    "             'sd_interactions_per_session', 'sd_purchases_per_session']\n",
    "\n",
    "\n",
    "outputCol = \"T_total_spend\"\n",
    "\n",
    "pipeline = generateLinearPipeline(inputCols, outputCol)\n",
    "pipelineModel = pipeline.fit(part_2_train)\n",
    "\n",
    "print(\"Model coefficients\")\n",
    "print(modelInfo(inputCols, pipelineModel))\n",
    "\n",
    "evaluationMetrics = getLinearEvaluationMetrics(pipelineModel,outputCol,part_2_test, inputCols)\n",
    "print(f\"RMSE is {evaluationMetrics[0]:.1f}\")\n",
    "print(f\"R^2 is {evaluationMetrics[1]:.5f}\")\n",
    "print(f\"Adjusted R^2 is {evaluationMetrics[2]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluationMetrics[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "|T_total_spend|   FinalPredictions|\n",
      "+-------------+-------------------+\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0| 257221.07310688792|\n",
      "|          0.0|                0.0|\n",
      "|          0.0| -24597.82746776846|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|  94746.70316586451|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0| 227085.03585039263|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0| 137944.05606344837|\n",
      "|          0.0| -53492.67417208022|\n",
      "|          0.0| 235382.79038782284|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0| 55394.643886670376|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0| 47444.698245930835|\n",
      "|          0.0|-23630.464115384464|\n",
      "|          0.0|                0.0|\n",
      "|          0.0| 166887.68444121472|\n",
      "|          0.0|  60745.48986036662|\n",
      "|          0.0|-127998.50912384622|\n",
      "|          0.0|  277146.5052513584|\n",
      "|          0.0|  157134.5623980453|\n",
      "|          0.0| 160341.02318022205|\n",
      "|          0.0|  64511.79301534959|\n",
      "|          0.0| 173124.88955535725|\n",
      "|          0.0|  93252.80569012837|\n",
      "|          0.0|  66659.00664312892|\n",
      "|          0.0|  70247.25384319873|\n",
      "|          0.0|  65013.36277627167|\n",
      "|          0.0| 191237.82653355118|\n",
      "|          0.0|                0.0|\n",
      "|          0.0| 125445.81502766178|\n",
      "|          0.0|  85894.87373000183|\n",
      "|          0.0|  74214.20500131976|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0| 62608.367969986044|\n",
      "|          0.0|  44391.44512946483|\n",
      "|          0.0| 185145.34507415126|\n",
      "|          0.0|  96112.38502187547|\n",
      "|          0.0| 119217.56247083332|\n",
      "|          0.0| 193785.71776791505|\n",
      "|          0.0|  66159.84203488719|\n",
      "|          0.0| 141282.49643088324|\n",
      "|          0.0|  93730.46811034388|\n",
      "|          0.0|  52509.44956792524|\n",
      "|          0.0|  29763.64283303925|\n",
      "|          0.0| 132480.94431017982|\n",
      "|          0.0|  17767.65882587026|\n",
      "|          0.0| 46165.635404758716|\n",
      "|          0.0|   69073.7015562304|\n",
      "|          0.0|  53395.53183356861|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|                0.0|\n",
      "|          0.0|  32140.17301966856|\n",
      "|          0.0| -26311.07589349039|\n",
      "|          0.0|  122268.9913856068|\n",
      "|          0.0| -931.5629642908947|\n",
      "|          0.0|  59651.19989817551|\n",
      "|          0.0| 106006.83496876423|\n",
      "|          0.0|  74223.84684352527|\n",
      "|          0.0|-14580.227312630277|\n",
      "|          0.0|  145630.1066110437|\n",
      "|          0.0|  74587.78571115714|\n",
      "|          0.0|   71911.0015059834|\n",
      "|          0.0|  93003.70953974112|\n",
      "|          0.0|  44132.60702476657|\n",
      "+-------------+-------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('T_total_spend', \"FinalPredictions\").sort(\"t_total_spend\").show(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
