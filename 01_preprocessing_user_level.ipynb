{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File 01 - User-level Preprocessed Data Output\n",
    "\n",
    "In this file, we create the preprocessed user-level data with a few features.\n",
    "\n",
    "Outputs:\n",
    "- The preprocessed data file is output to `processed_data/preprocessed_01.parquet`.\n",
    "- Additionally, the raw data is filtered on the preprocessed data user ids: `processed_data/month_01_filtered.parquet`, `processed_data/month_02_filtered.parquet`.\n",
    "\n",
    "And then in script 02, we add additional features to the preprocessed data table for every row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Spark session and data schema\n",
    "\n",
    "We can specify more options in the SparkSession creator, but currently the options are at the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.76 ms, sys: 8.73 ms, total: 18.5 ms\n",
      "Wall time: 18.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "schema = \"`event_time` TIMESTAMP,`event_type` STRING,`product_id` INT,`category_id` BIGINT,`category_code` STRING,`brand` STRING,`price` FLOAT,`user_id` INT,`user_session` STRING\"\n",
    "#ddl_schema = T._parse_datatype_string(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.34 ms, sys: 2.34 ms, total: 5.68 ms\n",
      "Wall time: 1.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df1 = spark.read.schema(schema).csv(\"/project/ds5559/group12/raw_data/2020-01.csv\")\n",
    "df2 = spark.read.schema(schema).csv(\"/project/ds5559/group12/raw_data/2020-02.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit number of records in dataframes\n",
    "\n",
    "We can limit each dataframe to a smaller subset. Notably, the dataframe is arranged by time, so this is how the subset will be biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1=df1.limit(100000)\n",
    "df1.createOrReplaceTempView(\"m1\")\n",
    "\n",
    "# df2=df11.limit(100000)\n",
    "df2.createOrReplaceTempView(\"m2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a transformed table containing elements of interest for our model\n",
    "\n",
    "We propose a basic table format (see https://docs.google.com/document/d/1NG4KGticBXn0D3PL5_zMxLV2Pr7A8PQtLcasxCOd1nA/edit).\n",
    "\n",
    "Every row is a user_id who exists in M1 and may or may not exist in M2.\n",
    "Columns include:\n",
    "- `user_id` (ID)\n",
    "- `m2_total_spend` (sum among all purchase events, NB: month 2. Will be 0 is user does not exist in month 2 or makes no purchase events)\n",
    "- `m1_total_spend` (sum among all purchase events)\n",
    "- `m1_user_sessions` (count of distinct user sessions/browsing sessions)\n",
    "- `m1_purchase_events` (count of distinct purchase events)\n",
    "\n",
    "Additional rows may have been added below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+------------+--------------+\n",
      "|  user_id|       T_total_spend|         total_spend|total_events|total_sessions|\n",
      "+---------+--------------------+--------------------+------------+--------------+\n",
      "|568782581|2.3885857005200386E8| 5.821735228239441E7|      926856|           532|\n",
      "|582826305|1.9694101198085403E8|1.7104022296691895E7|      605784|           123|\n",
      "|563599039|2.3802253637968063E8| 6.886928014480591E7|      442758|           108|\n",
      "|568805468|  2.09597173788414E8|  3391851.5849990845|      425169|          4636|\n",
      "|592727922|1.1673367678547668E8|   8990452.824829102|      423648|            78|\n",
      "+---------+--------------------+--------------------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 5.75 ms, sys: 7.08 ms, total: 12.8 ms\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.sql(\"\"\"SELECT\n",
    "\n",
    "               /* ID */\n",
    "               m1.user_id AS user_id,\n",
    "               \n",
    "               /* Output: Total spend in month 2; Will be 0 is user does not exist in month 2 or makes no purchase events */\n",
    "               IFNULL(SUM(m2.price),0) AS T_total_spend,\n",
    "               \n",
    "               \n",
    "               /* Total spend in month 1: we sum the price of any 'purchase' events */\n",
    "               SUM(CAST(m1.event_type=='purchase' AS INT) * m1.price) AS total_spend,\n",
    "               \n",
    "               /* Total events in month 1: includes all event types (multiple per session) */\n",
    "               COUNT(m1.event_type) AS total_events,\n",
    "               \n",
    "               /* Total purchase events in month 1: we sum the occurence of any 'purchase' events */\n",
    "               /* SUM(CAST(m1.event_type=='purchase' AS INT)) AS purchase_events, */\n",
    "               \n",
    "               /* Total user sessions in month 1: we count all distinct user sessions */\n",
    "               COUNT(DISTINCT m1.user_session) AS total_sessions\n",
    "               \n",
    "               FROM m1\n",
    "            \n",
    "            /* Note: This is a left join, so purchasers in month 2 must be in month 1 to be included in the output */\n",
    "            LEFT JOIN \n",
    "            (\n",
    "               SELECT * FROM m2\n",
    "               WHERE event_type='purchase'\n",
    "            ) m2\n",
    "            \n",
    "            ON m1.user_id=m2.user_id\n",
    "            \n",
    "            /* Prevent adding bad data where user_id is null */\n",
    "            WHERE ISNULL(m1.user_id)<>1\n",
    "            \n",
    "            GROUP BY m1.user_id ORDER BY total_events DESC\"\"\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove customers who did not make a purchase in month 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(col('total_spend') > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter the overall dataset to a subset in order to make sure that the output from the feature addition script is under 3GB; then write to Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36228\n",
      "+---------+------------------+------------------+------------+--------------+\n",
      "|  user_id|     T_total_spend|       total_spend|total_events|total_sessions|\n",
      "+---------+------------------+------------------+------------+--------------+\n",
      "|512479812|50881.730712890625|166.19000244140625|          73|             9|\n",
      "|512562561|               0.0| 38.33000183105469|          82|            11|\n",
      "|512583155|2888.1599731445312|443.65999603271484|          22|             4|\n",
      "|512996234|               0.0| 81.95999908447266|          12|             4|\n",
      "|513143907|               0.0|  965.219970703125|          15|             2|\n",
      "+---------+------------------+------------------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 12.7 ms, sys: 11.3 ms, total: 24 ms\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# We can control this level to make the preprocessed data under 3 GB\n",
    "split_level = 0.1 # percentage\n",
    "\n",
    "kept_df, garbage_df = df.randomSplit([split_level, 1-split_level], seed=42)\n",
    "kept_df.write.mode(\"overwrite\").parquet(\"./processed_data/preprocessed_01.parquet\")\n",
    "print(kept_df.count())\n",
    "kept_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.72 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Not necessary at this time, but this CSV can be written if desired\n",
    "# kept_df.coalesce(1).write.option(\"header\", \"true\").csv(\"./processed_data/temp_preprocessed_01.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the raw data, filtered on the appropriate user-ids, to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1598123\n",
      "+---------+-------------------+----------+----------+-------------------+--------------------+------+------+--------------------+\n",
      "|  user_id|         event_time|event_type|product_id|        category_id|       category_code| brand| price|        user_session|\n",
      "+---------+-------------------+----------+----------+-------------------+--------------------+------+------+--------------------+\n",
      "|512479812|2020-01-02 17:46:13|      view|   1004205|2053013563835941749|appliances.kitche...|xiaomi|205.67|4fcdf9e6-0ce4-4c3...|\n",
      "|512479812|2020-01-02 17:46:42|      view|   1004402|2053013555631882655|electronics.smart...|xiaomi|246.79|4fcdf9e6-0ce4-4c3...|\n",
      "|512479812|2020-01-02 17:47:07|      view|   1005159|2232732093077520756|construction.tool...|xiaomi|166.03|4fcdf9e6-0ce4-4c3...|\n",
      "|512479812|2020-01-02 17:47:29|      view|   1005161|2232732093077520756|construction.tool...|xiaomi|177.35|4fcdf9e6-0ce4-4c3...|\n",
      "|512479812|2020-01-02 17:49:58|      view|  49800004|2053013556168753601|       apparel.shirt|  null|  39.9|4fcdf9e6-0ce4-4c3...|\n",
      "+---------+-------------------+----------+----------+-------------------+--------------------+------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 29.4 ms, sys: 24.4 ms, total: 53.8 ms\n",
      "Wall time: 6min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "month_01_filtered = df1.join(kept_df,'user_id','leftsemi')\n",
    "print(month_01_filtered.count())\n",
    "month_01_filtered.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 2 µs, total: 4 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Probably not actually needed\n",
    "#month_02_filtered = df2.join(kept_df,'user_id','leftsemi')\n",
    "#print(month_02_filtered.count())\n",
    "#month_02_filtered.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.7 ms, sys: 22.5 ms, total: 50.2 ms\n",
      "Wall time: 6min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "month_01_filtered.write.mode(\"overwrite\").parquet(\"./processed_data/month_01_filtered.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# month_02_filtered.write.mode(\"overwrite\").parquet(\"./processed_data/month_02_filtered.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
