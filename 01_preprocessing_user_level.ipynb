{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File 01 - Preprocessed Data Output (Step 1)\n",
    "##### Group 12:\n",
    "\n",
    "##### Hannah Schmuckler, mmc4cv\n",
    "\n",
    "##### Rob Schwartz, res7cd\n",
    "\n",
    "In this file, we create the scaffolding of the preprocessed user-level data from event-level data, with a few features. The event-level data is given with 1 record equal to one interaction, while the user-level data is given with 1 record equal to one user. Each row in the output data represents one user who exists in the first month.\n",
    "\n",
    "Outputs:\n",
    "- The preprocessed data file is output to `processed_data/preprocessed_01.parquet`.\n",
    "- Additionally, the raw data is filtered on the preprocessed data user ids: `processed_data/month_01_filtered.parquet`, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Spark session and data schema\n",
    "\n",
    "We can specify more options in the SparkSession creator, but currently the options are at the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 188 ms, sys: 157 ms, total: 345 ms\n",
      "Wall time: 4.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "schema = \"`event_time` TIMESTAMP,`event_type` STRING,`product_id` INT,`category_id` BIGINT,`category_code` STRING,`brand` STRING,`price` FLOAT,`user_id` INT,`user_session` STRING\"\n",
    "#ddl_schema = T._parse_datatype_string(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.1 ms, sys: 1.36 ms, total: 4.46 ms\n",
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Be sure that you have downloaded the data from either\n",
    "# - https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store\n",
    "# - https://drive.google.com/drive/folders/1Nan8X33H8xrXS5XhCKZmSpClFTCJsSpE\n",
    "# Then we are using January and Febrary 2020.\n",
    "\n",
    "df1 = spark.read.schema(schema).csv(\"/project/ds5559/group12/raw_data/2020-01.csv\")\n",
    "df2 = spark.read.schema(schema).csv(\"/project/ds5559/group12/raw_data/2020-02.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create temp views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Spark SQL\n",
    "df1.createOrReplaceTempView(\"m1\")\n",
    "df2.createOrReplaceTempView(\"m2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a transformed table containing elements of interest for our model\n",
    "\n",
    "We propose a basic table format (see https://docs.google.com/document/d/1NG4KGticBXn0D3PL5_zMxLV2Pr7A8PQtLcasxCOd1nA/edit).\n",
    "\n",
    "Every row is a user_id who exists in M1 and may or may not exist in M2.\n",
    "Columns include:\n",
    "- `user_id` (ID)\n",
    "- `total_spend` (sum among all purchase events in month 1)\n",
    "- `total_events` (count of distinct user actions during month 1)\n",
    "- `user_sessions` (count of distinct user sessions/browsing sessions during month 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+------------+--------------+\n",
      "|  user_id|      total_spend|total_events|total_sessions|\n",
      "+---------+-----------------+------------+--------------+\n",
      "|597644399|              0.0|       41280|         40188|\n",
      "|569335945|              0.0|       23058|         23057|\n",
      "|594718064|              0.0|       16347|         15890|\n",
      "|597514055|              0.0|       13717|         12952|\n",
      "|568804062|172.9600067138672|       11479|          9149|\n",
      "+---------+-----------------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 7.55 ms, sys: 70.5 ms, total: 78 ms\n",
      "Wall time: 1min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4385985"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.sql(\"\"\"SELECT\n",
    "\n",
    "               /* ID */\n",
    "               m1.user_id AS user_id,\n",
    "               \n",
    "               /* Total spend in month 1: we sum the price of any 'purchase' events */\n",
    "               SUM(CAST(m1.event_type=='purchase' AS INT) * m1.price) AS total_spend,\n",
    "               \n",
    "               /* Total events in month 1: includes all event types (multiple per session) */\n",
    "               COUNT(m1.event_type) AS total_events,\n",
    "               \n",
    "               /* Total user sessions in month 1: we count all distinct user sessions */\n",
    "               COUNT(DISTINCT m1.user_session) AS total_sessions\n",
    "               \n",
    "               FROM m1\n",
    "               \n",
    "            /* Note: This is a left join, so purchasers in month 2 must be in month 1 to be included in the output */\n",
    "            LEFT JOIN\n",
    "            (\n",
    "               SELECT\n",
    "                 user_id,\n",
    "                 SUM(m2.price) AS price FROM m2\n",
    "                 \n",
    "               WHERE event_type='purchase'\n",
    "               GROUP BY m2.user_id\n",
    "            ) m2\n",
    "           \n",
    "            ON m1.user_id=m2.user_id\n",
    "           \n",
    "            /* Prevent adding bad data where user_id is null */\n",
    "            WHERE ISNULL(m1.user_id)<>1\n",
    "\n",
    "            GROUP BY m1.user_id ORDER BY total_events DESC\n",
    "           \n",
    "            \"\"\")\n",
    "df.show(5)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove customers who did not make a purchase in month 1.\n",
    "We do this because it is unlikely that we will be able to actionably influence customers who don't make a purchase in month 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(col('total_spend') > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359105"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create response variable (individual level) of total spend in month 2 (T_total_spend), join with month 1 data, and fill nulls with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_spend_response = df2.filter(col('event_type') == \"purchase\").groupBy(col('user_id')).sum('price').withColumnRenamed('sum(price)', \"T_total_spend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 567 µs, sys: 1.83 ms, total: 2.4 ms\n",
      "Wall time: 42.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = df.join(total_spend_response, df.user_id == total_spend_response.user_id, 'leftouter').drop(total_spend_response.user_id)\n",
    "\n",
    "#df.show()\n",
    "#df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0.0, \"T_total_spend\")\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"./processed_data/preprocessed_01.parquet\")\n",
    "#print(df.count())\n",
    "#df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 2 µs, total: 3 µs\n",
      "Wall time: 6.44 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Not necessary at this time, but this CSV can be written if desired\n",
    "# kept_df.coalesce(1).write.option(\"header\", \"true\").csv(\"./processed_data/temp_preprocessed_01.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the raw data, filtered on the appropriate user-ids, to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 904 µs, total: 904 µs\n",
      "Wall time: 12.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# month_01_filtered = df1.join(kept_df,'user_id','leftsemi')\n",
    "month_01_filtered = df1.join(df,'user_id','leftsemi')\n",
    "#print(month_01_filtered.count())\n",
    "#month_01_filtered.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15923973"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_01_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 ms, sys: 45.5 ms, total: 55.7 ms\n",
      "Wall time: 3min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "month_01_filtered.write.mode(\"overwrite\").parquet(\"./processed_data/month_01_filtered.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
