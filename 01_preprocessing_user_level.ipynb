{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File 01 - User-level Preprocessed Data Output\n",
    "\n",
    "In this file, we create the preprocessed user-level data with a few features.\n",
    "\n",
    "Outputs:\n",
    "- The preprocessed data file is output to `processed_data/preprocessed_01.parquet`.\n",
    "- Additionally, the raw data is filtered on the preprocessed data user ids: `processed_data/month_01_filtered.parquet`, `processed_data/month_02_filtered.parquet`.\n",
    "\n",
    "And then in script 02, we add additional features to the preprocessed data table for every row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Spark session and data schema\n",
    "\n",
    "We can specify more options in the SparkSession creator, but currently the options are at the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.64 ms, sys: 0 ns, total: 1.64 ms\n",
      "Wall time: 15.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "schema = \"`event_time` TIMESTAMP,`event_type` STRING,`product_id` INT,`category_id` BIGINT,`category_code` STRING,`brand` STRING,`price` FLOAT,`user_id` INT,`user_session` STRING\"\n",
    "#ddl_schema = T._parse_datatype_string(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.14 ms, sys: 1.1 ms, total: 4.24 ms\n",
      "Wall time: 81.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df1 = spark.read.schema(schema).csv(\"/project/ds5559/group12/raw_data/2020-01.csv\")\n",
    "df2 = spark.read.schema(schema).csv(\"/project/ds5559/group12/raw_data/2020-02.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit number of records in dataframes\n",
    "\n",
    "We can limit each dataframe to a smaller subset. Notably, the dataframe is arranged by time, so this is how the subset will be biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1=df1.limit(100000)\n",
    "df1.createOrReplaceTempView(\"m1\")\n",
    "\n",
    "# df2=df11.limit(100000)\n",
    "df2.createOrReplaceTempView(\"m2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a transformed table containing elements of interest for our model\n",
    "\n",
    "We propose a basic table format (see https://docs.google.com/document/d/1NG4KGticBXn0D3PL5_zMxLV2Pr7A8PQtLcasxCOd1nA/edit).\n",
    "\n",
    "Every row is a user_id who exists in M1 and may or may not exist in M2.\n",
    "Columns include:\n",
    "- `user_id` (ID)\n",
    "- `m2_total_spend` (sum among all purchase events, NB: month 2. Will be 0 is user does not exist in month 2 or makes no purchase events)\n",
    "- `m1_total_spend` (sum among all purchase events)\n",
    "- `m1_user_sessions` (count of distinct user sessions/browsing sessions)\n",
    "- `m1_purchase_events` (count of distinct purchase events)\n",
    "\n",
    "Additional rows may have been added below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+------------+--------------+\n",
      "|  user_id|       T_total_spend|         total_spend|total_events|total_sessions|\n",
      "+---------+--------------------+--------------------+------------+--------------+\n",
      "|568782581|2.3885857005200386E8| 5.821735228239441E7|      926856|           532|\n",
      "|582826305|1.9694101198085403E8|1.7104022296691895E7|      605784|           123|\n",
      "|563599039|2.3802253637968063E8| 6.886928014480591E7|      442758|           108|\n",
      "|568805468|  2.09597173788414E8|  3391851.5849990845|      425169|          4636|\n",
      "|592727922|1.1673367678547668E8|   8990452.824829102|      423648|            78|\n",
      "+---------+--------------------+--------------------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 13.6 ms, sys: 12.4 ms, total: 26 ms\n",
      "Wall time: 2min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.sql(\"\"\"SELECT\n",
    "\n",
    "               /* ID */\n",
    "               m1.user_id AS user_id,\n",
    "               \n",
    "               /* Output: Total spend in month 2; Will be 0 is user does not exist in month 2 or makes no purchase events */\n",
    "               IFNULL(SUM(m2.price),0) AS T_total_spend,\n",
    "               \n",
    "               \n",
    "               /* Total spend in month 1: we sum the price of any 'purchase' events */\n",
    "               SUM(CAST(m1.event_type=='purchase' AS INT) * m1.price) AS total_spend,\n",
    "               \n",
    "               /* Total events in month 1: includes all event types (multiple per session) */\n",
    "               COUNT(m1.event_type) AS total_events,\n",
    "               \n",
    "               /* Total purchase events in month 1: we sum the occurence of any 'purchase' events */\n",
    "               /* SUM(CAST(m1.event_type=='purchase' AS INT)) AS purchase_events, */\n",
    "               \n",
    "               /* Total user sessions in month 1: we count all distinct user sessions */\n",
    "               COUNT(DISTINCT m1.user_session) AS total_sessions\n",
    "               \n",
    "               FROM m1\n",
    "            \n",
    "            /* Note: This is a left join, so purchasers in month 2 must be in month 1 to be included in the output */\n",
    "            LEFT JOIN \n",
    "            (\n",
    "               SELECT * FROM m2\n",
    "               WHERE event_type='purchase'\n",
    "            ) m2\n",
    "            \n",
    "            ON m1.user_id=m2.user_id\n",
    "            \n",
    "            /* Prevent adding bad data where user_id is null */\n",
    "            WHERE ISNULL(m1.user_id)<>1\n",
    "            \n",
    "            GROUP BY m1.user_id ORDER BY total_events DESC\"\"\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter the overall dataset to a subset in order to make sure that the output from the feature addition script is under 3GB; then write to Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219080\n",
      "+---------+-------------+-----------+------------+--------------+\n",
      "|  user_id|T_total_spend|total_spend|total_events|total_sessions|\n",
      "+---------+-------------+-----------+------------+--------------+\n",
      "|405614124|          0.0|        0.0|           2|             2|\n",
      "|485991194|          0.0|        0.0|           3|             1|\n",
      "|496765250|          0.0|        0.0|           1|             1|\n",
      "|501980918|          0.0|        0.0|        2141|            40|\n",
      "|502621333|          0.0|        0.0|           5|             3|\n",
      "+---------+-------------+-----------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 15.5 ms, sys: 11 ms, total: 26.5 ms\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# We can control this level to make the preprocessed data under 3 GB\n",
    "split_level = 0.05 # percentage\n",
    "\n",
    "kept_df, garbage_df = df.randomSplit([split_level, 1-split_level], seed=42)\n",
    "kept_df.write.mode(\"overwrite\").parquet(\"./processed_data/preprocessed_01.parquet\")\n",
    "print(kept_df.count())\n",
    "kept_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Not necessary at this time, but this CSV can be written if desired\n",
    "# kept_df.coalesce(1).write.option(\"header\", \"true\").csv(\"./processed_data/temp_preprocessed_01.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the raw data, filtered on the appropriate user-ids, to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2807167\n",
      "+---------+-------------------+----------+----------+-------------------+--------------------+-------+------+--------------------+\n",
      "|  user_id|         event_time|event_type|product_id|        category_id|       category_code|  brand| price|        user_session|\n",
      "+---------+-------------------+----------+----------+-------------------+--------------------+-------+------+--------------------+\n",
      "|405614124|2020-01-06 11:42:08|      view|   1004767|2232732093077520756|construction.tool...|samsung|232.77|18bc9e5e-d556-4cd...|\n",
      "|405614124|2020-01-23 21:47:06|      view|   4804718|2232732079706079299|       sport.bicycle|  apple|321.41|7e972da4-7007-46d...|\n",
      "|485991194|2020-01-06 18:35:24|      view|   1004767|2232732093077520756|construction.tool...|samsung|232.67|a7ac67d4-bec6-42b...|\n",
      "|485991194|2020-01-06 18:41:44|      view|   1004767|2232732093077520756|construction.tool...|samsung|232.67|a7ac67d4-bec6-42b...|\n",
      "|485991194|2020-01-06 18:42:02|      view|   1004767|2232732093077520756|construction.tool...|samsung|232.67|a7ac67d4-bec6-42b...|\n",
      "+---------+-------------------+----------+----------+-------------------+--------------------+-------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 51.7 ms, sys: 63.9 ms, total: 116 ms\n",
      "Wall time: 11min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "month_01_filtered = df1.join(kept_df,'user_id','leftsemi')\n",
    "print(month_01_filtered.count())\n",
    "month_01_filtered.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably not actually needed\n",
    "%%time\n",
    "month_02_filtered = df2.join(kept_df,'user_id','leftsemi')\n",
    "print(month_02_filtered.count())\n",
    "month_02_filtered.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.5 ms, sys: 29.8 ms, total: 58.3 ms\n",
      "Wall time: 6min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "month_01_filtered.write.mode(\"overwrite\").parquet(\"./processed_data/month_01_filtered.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# month_02_filtered.write.mode(\"overwrite\").parquet(\"./processed_data/month_02_filtered.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
